{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "## Char RNN 生成文本\n",
        "In the chapter on Cyclic Neural Networks, we learned that it is very good at dealing with sequence problems, so for text, it is also equivalent to a sequence, because each sentence is composed of words or man in sequence order, so we can also use RNN processes it, so how can it generate text? In fact, the principle is very simple, let's talk about Char RNN.\n",
        "\n",
        "### Training process\n",
        "Earlier we introduced that there are many relationships between input and output of RNN, such as one-to-many, many-to-many, etc. Different inputs correspond to different applications, such as many-to-many can be used for machine translation, etc. Today we want The Char RNN is a many-to-many type of the same length when training the network, that is, inputting a sequence and outputting a sequence of absorption common length.\n",
        "\n",
        "The specific network training process is as follows\n",
        "\n",
        "<img src=https://ws1.sinaimg.cn/large/006tNc79gy1fob5kq3r8jj30mt09dq2r.jpg width=700>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "As you can see in the above network flow, the input is a sequence of \"front moonlight\" and the output is also a sequence of \"previous moonlight bed\". If you look closely, you can find that every step of the network output is the next step, is this a coincidence?\n",
        "\n",
        "No, this is Char RNN's design idea. For any sentence, such as \"I like kittens\", we can split it into Char RNN's training set. The input is \"I like kittens\". A sequence of length 5, the output of each step of the network is \"like the kitten me.\" Of course, for a sequence, there is no other character after the last character, so there are many ways to choose, such as the first character of the sequence as its output, that is, the output of \"light\" is \"bed\", or it will be The output itself, that is, the output of \"light\" is \"light.\"\n",
        "\n",
        "What are the benefits of this design? Because the process of training is a process of supervised training, we can't see the meaning of doing so. We can see the benefits of doing this in the process of generating text.\n",
        "\n",
        "### Generate text\n",
        "We can directly explain the process of generating text, and can explain the reason of the training process intuitively.\n",
        "\n",
        "First, you need to input the initial sequence of the network to warm up. The warm-up process does not require the actual output. Just to generate the hidden state with the memory effect, and keep the hidden state, then we start to form the text, continuously Generate a new sentence, this process can be looped indefinitely, or reach the length of our request output, you can look at the following icon\n",
        "\n",
        "<img src=https://ws2.sinaimg.cn/large/006tNc79gy1fob5z06w1uj30qh09m0sl.jpg width=800>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "As you can see from the above example, is it easy to re-enter the previously outputted text into the network, looping through the recursion, and finally generating the sentences of the length we want?\n",
        "\n",
        "Below we use PyTorch to achieve\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "We use ancient poetry as an example, read this data and see what it looks like.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-18T03:28:52.315656Z",
          "start_time": "2018-02-18T03:28:52.286844Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "with open('./dataset/poetry.txt', 'r') as f:\n",
        "    poetry_corpus = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-18T03:28:52.331908Z",
          "start_time": "2018-02-18T03:28:52.317790Z"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'寒随穷律变，春逐鸟声开。\\n初风飘带柳，晚雪间花梅。\\n碧林青旧竹，绿沼翠新苔。\\n芝田初雁去，绮树巧莺来。\\n晚霞聊自怡，初晴弥可喜。\\n日晃百花色，风动千林翠。\\n池鱼跃不同，园鸟声还异。\\n寄言博通者，知予物'"
            ]
          },
          "execution_count": 2,
          "metadata": {
            
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "poetry_corpus[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-18T03:28:52.338277Z",
          "start_time": "2018-02-18T03:28:52.334069Z"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "942681"
            ]
          },
          "execution_count": 3,
          "metadata": {
            
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Look at the number of characters\n",
        "len(poetry_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "For the sake of visualization, we replaced some other characters with spaces.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-18T03:28:52.353185Z",
          "start_time": "2018-02-18T03:28:52.340405Z"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'寒随穷律变 春逐鸟声开  初风飘带柳 晚雪间花梅  碧林青旧竹 绿沼翠新苔  芝田初雁去 绮树巧莺来  晚霞聊自怡 初晴弥可喜  日晃百花色 风动千林翠  池鱼跃不同 园鸟声还异  寄言博通者 知予物'"
            ]
          },
          "execution_count": 4,
          "metadata": {
            
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "poetry_corpus = poetry_corpus.replace('\\n', ' ').replace('\\r', ' ').replace('，', ' ').replace('。', ' ')\n",
        "poetry_corpus[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "### Text numeric representation\n",
        "For each text, the computer does not recognize it effectively, so you must make a conversion to convert the text to a number. For all non-repeating characters, you can start indexing from 0.\n",
        "\n",
        "At the same time, in order to save memory overhead, words with lower word frequency can be removed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-18T03:28:52.642640Z",
          "start_time": "2018-02-18T03:28:52.355357Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class TextConverter(object):\n",
        "    def __init__(self, text_path, max_vocab=5000):\n",
        "\"\"\"Create a character index converter\n",
        "        \n",
        "        Args:\n",
        "Text_path: text position\n",
        "Max_vocab: the maximum number of words\n",
        "        \"\"\"\n",
        "        \n",
        "        with open(text_path, 'r') as f:\n",
        "            text = f.read()\n",
        "text = text.replace('\\n', ' ').replace('\\r', ' ').replace('，', ' ').replace('。', ' ')\n",
        "# Remove duplicate characters\n",
        "        vocab = set(text)\n",
        "\n",
        "# If the total number of words exceeds the maximum value, remove the lowest frequency\n",
        "        vocab_count = {}\n",
        "        \n",
        "# Calculate the frequency of occurrence of words and sort\n",
        "        for word in vocab:\n",
        "            vocab_count[word] = 0\n",
        "        for word in text:\n",
        "            vocab_count[word] += 1\n",
        "        vocab_count_list = []\n",
        "        for word in vocab_count:\n",
        "            vocab_count_list.append((word, vocab_count[word]))\n",
        "        vocab_count_list.sort(key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "# If the maximum value is exceeded, the character with the lowest interception frequency\n",
        "        if len(vocab_count_list) > max_vocab:\n",
        "            vocab_count_list = vocab_count_list[:max_vocab]\n",
        "        vocab = [x[0] for x in vocab_count_list]\n",
        "        self.vocab = vocab\n",
        "\n",
        "        self.word_to_int_table = {c: i for i, c in enumerate(self.vocab)}\n",
        "        self.int_to_word_table = dict(enumerate(self.vocab))\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.vocab) + 1\n",
        "\n",
        "    def word_to_int(self, word):\n",
        "        if word in self.word_to_int_table:\n",
        "            return self.word_to_int_table[word]\n",
        "        else:\n",
        "            return len(self.vocab)\n",
        "\n",
        "    def int_to_word(self, index):\n",
        "        if index == len(self.vocab):\n",
        "            return '<unk>'\n",
        "        elif index < len(self.vocab):\n",
        "            return self.int_to_word_table[index]\n",
        "        else:\n",
        "            raise Exception('Unknown index!')\n",
        "\n",
        "    def text_to_arr(self, text):\n",
        "        arr = []\n",
        "        for word in text:\n",
        "            arr.append(self.word_to_int(word))\n",
        "        return np.array(arr)\n",
        "\n",
        "    def arr_to_text(self, arr):\n",
        "        words = []\n",
        "        for index in arr:\n",
        "            words.append(self.int_to_word(index))\n",
        "        return \"\".join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-18T03:28:53.016322Z",
          "start_time": "2018-02-18T03:28:52.645616Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "convert = TextConverter('./dataset/poetry.txt', max_vocab=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "We can visualize the characters represented by numbers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-18T03:28:53.025196Z",
          "start_time": "2018-02-18T03:28:53.018514Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "寒随穷律变 春逐鸟声开\n",
            "[ 40 166 358 935 565   0  10 367 108  63  78]\n"
          ]
        }
      ],
      "source": [
        "#原文字字符\n",
        "txt_char = poetry_corpus[:11]\n",
        "print(txt_char)\n",
        "\n",
        "# Convert to numbers\n",
        "print(convert.text_to_arr(txt_char))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "### Constructing time series sample data\n",
        "In order to input into the cyclic neural network for training, we need to construct the data of the time series samples. Because we know that the cyclic neural network has long-term dependence problems, so we can't input all the texts together as a sequence to the circulating nerves. In the network, we need to divide the whole text into many sequences to make the batch input into the network. As long as we set the length of each sequence, the number of sequences is determined.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-18T03:28:53.036447Z",
          "start_time": "2018-02-18T03:28:53.027222Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "47134\n"
          ]
        }
      ],
      "source": [
        "n_step = 20\n",
        "\n",
        "# total number of sequences\n",
        "num_seq = int(len(poetry_corpus) / n_step)\n",
        "\n",
        "# Remove the last part of the sequence length\n",
        "text = poetry_corpus[:num_seq*n_step]\n",
        "\n",
        "print(num_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Then we convert all the text in the sequence into a digital representation and rearrange it into a matrix of (num_seq x n_step)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-18T03:28:53.258155Z",
          "start_time": "2018-02-18T03:28:53.038479Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-18T03:28:53.921749Z",
          "start_time": "2018-02-18T03:28:53.260507Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([47134, 20])\n",
            "\n",
            "  40\n",
            " 166\n",
            " 358\n",
            " 935\n",
            " 565\n",
            "   0\n",
            "  10\n",
            " 367\n",
            " 108\n",
            "  63\n",
            "  78\n",
            "   0\n",
            "   0\n",
            " 150\n",
            "   4\n",
            " 443\n",
            " 284\n",
            " 182\n",
            "   0\n",
            " 131\n",
            "[torch.LongTensor of size 20]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "arr = convert.text_to_arr(text)\n",
        "arr = arr.reshape((num_seq, -1))\n",
        "arr = torch.from_numpy(arr)\n",
        "\n",
        "print(arr.shape)\n",
        "print(arr[0, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "According to this, we can construct the data reading in PyTorch to train the network. Here we set the output label of the last character as the first character of the input, that is, the output of \"Before the Moonlight\" is \"the former moonlight bed.\" \"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-18T03:28:53.945768Z",
          "start_time": "2018-02-18T03:28:53.925488Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "class TextDataset(object):\n",
        "    def __init__(self, arr):\n",
        "        self.arr = arr\n",
        "        \n",
        "    def __getitem__(self, item):\n",
        "        x = self.arr[item, :]\n",
        "        \n",
        "#结构 label\n",
        "        y = torch.zeros(x.shape)\n",
        "# The first character entered is the last input label\n",
        "        y[:-1], y[-1] = x[1:], x[0]\n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.arr.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-18T03:28:53.950296Z",
          "start_time": "2018-02-18T03:28:53.947697Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "train_set = TextDataset(arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "We can take out one of the data sets and see if it is what we described.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-18T03:28:53.957705Z",
          "start_time": "2018-02-18T03:28:53.952232Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "寒随穷律变 春逐鸟声开  初风飘带柳 晚\n",
            "随穷律变 春逐鸟声开  初风飘带柳 晚寒\n"
          ]
        }
      ],
      "source": [
        "x, y = train_set[0]\n",
        "print(convert.arr_to_text(x.numpy()))\n",
        "print(convert.arr_to_text(y.numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "### Modeling\n",
        "The model can be defined as a very simple three-layer, the first layer is the word embedding, the second layer is the RNN layer, because the last is a classification problem, so the third layer is the linear layer, and finally the predicted characters are output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-18T03:28:54.022455Z",
          "start_time": "2018-02-18T03:28:53.959687Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "use_gpu = True\n",
        "\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, num_classes, embed_dim, hidden_size, \n",
        "                 num_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.word_to_vec = nn.Embedding(num_classes, embed_dim)\n",
        "        self.rnn = nn.GRU(embed_dim, hidden_size, num_layers, dropout)\n",
        "        self.project = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, hs=None):\n",
        "        batch = x.shape[0]\n",
        "        if hs is None:\n",
        "            hs = Variable(\n",
        "                torch.zeros(self.num_layers, batch, self.hidden_size))\n",
        "            if use_gpu:\n",
        "                hs = hs.cuda()\n",
        "        word_embed = self.word_to_vec(x)  # (batch, len, embed)\n",
        "        word_embed = word_embed.permute(1, 0, 2)  # (len, batch, embed)\n",
        "        out, h0 = self.rnn(word_embed, hs)  # (len, batch, hidden)\n",
        "        le, mb, hd = out.shape\n",
        "        out = out.view(le * mb, hd)\n",
        "        out = self.project(out)\n",
        "        out = out.view(le, mb, -1)\n",
        "        out = out.permute(1, 0, 2).contiguous()  # (batch, len, hidden)\n",
        "        return out.view(-1, out.shape[2]), h0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "### Training Model\n",
        "When training the model, we know that this is a classification problem, so you can use cross entropy as the loss function. In the language model, we usually use a new indicator to evaluate the result. This indicator is called perplexity and can be simple. The ground is considered to take the exponent of the cross entropy, so its range is $[1, \\infty]$, and the smaller the better.\n",
        "\n",
        "In addition, we mentioned earlier that RNN has a gradient explosion problem, so we need to perform gradient clipping, which can be easily implemented in pytorch using `torch.nn.utils.clip_grad_norm`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-18T03:28:54.030508Z",
          "start_time": "2018-02-18T03:28:54.024511Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 128\n",
        "train_data = DataLoader(train_set, batch_size, True, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-18T03:28:59.955521Z",
          "start_time": "2018-02-18T03:28:54.032512Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from mxtorch.trainer import ScheduledOptim\n",
        "\n",
        "model = CharRNN(convert.vocab_size, 512, 512, 2, 0.5)\n",
        "if use_gpu:\n",
        "    model = model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "basic_optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "optimizer = ScheduledOptim(basic_optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-18T03:31:48.754799Z",
          "start_time": "2018-02-18T03:28:59.957657Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1, perplexity is: 290.865, lr:1.0e-03\n",
            "epoch: 2, perplexity is: 190.468, lr:1.0e-03\n",
            "epoch: 3, perplexity is: 124.909, lr:1.0e-03\n",
            "epoch: 4, perplexity is: 88.715, lr:1.0e-03\n",
            "epoch: 5, perplexity is: 67.819, lr:1.0e-03\n",
            "epoch: 6, perplexity is: 53.798, lr:1.0e-03\n",
            "epoch: 7, perplexity is: 43.619, lr:1.0e-03\n",
            "epoch: 8, perplexity is: 36.032, lr:1.0e-03\n",
            "epoch: 9, perplexity is: 30.195, lr:1.0e-03\n",
            "epoch: 10, perplexity is: 25.569, lr:1.0e-03\n",
            "epoch: 11, perplexity is: 21.868, lr:1.0e-03\n",
            "epoch: 12, perplexity is: 18.918, lr:1.0e-03\n",
            "epoch: 13, perplexity is: 16.482, lr:1.0e-03\n",
            "epoch: 14, perplexity is: 14.505, lr:1.0e-03\n",
            "epoch: 15, perplexity is: 12.870, lr:1.0e-03\n",
            "epoch: 16, perplexity is: 11.489, lr:1.0e-03\n",
            "epoch: 17, perplexity is: 10.358, lr:1.0e-03\n",
            "epoch: 18, perplexity is: 9.416, lr:1.0e-03\n",
            "epoch: 19, perplexity is: 8.619, lr:1.0e-03\n",
            "epoch: 20, perplexity is: 7.905, lr:1.0e-03\n"
          ]
        }
      ],
      "source": [
        "epochs = 20\n",
        "for e in range(epochs):\n",
        "    train_loss = 0\n",
        "    for data in train_data:\n",
        "        x, y = data\n",
        "        y = y.long()\n",
        "        if use_gpu:\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "        x, y = Variable(x), Variable(y)\n",
        "\n",
        "        # Forward.\n",
        "        score, _ = model(x)\n",
        "        loss = criterion(score, y.view(-1))\n",
        "\n",
        "        # Backward.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # Clip gradient.\n",
        "        nn.utils.clip_grad_norm(model.parameters(), 5)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.data[0]\n",
        "    print('epoch: {}, perplexity is: {:.3f}, lr:{:.1e}'.format(e+1, np.exp(train_loss / len(train_data)), optimizer.lr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "It can be seen that after training the model, we are able to reach a level of confusion of around 2.72, and we can start generating text below.\n",
        "\n",
        "### Generate text\n",
        "The process of generating text is very simple, as I mentioned earlier, given the starting characters, and then constantly generating characters backwards, passing the generated characters as new input to the network.\n",
        "\n",
        "It should be noted here that in order to increase more randomness, we will randomly select the probabilities based on their probabilities in the top five with the highest probability of prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-18T03:31:48.770181Z",
          "start_time": "2018-02-18T03:31:48.758123Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def pick_top_n(preds, top_n=5):\n",
        "    top_pred_prob, top_pred_label = torch.topk(preds, top_n, 1)\n",
        "    top_pred_prob /= torch.sum(top_pred_prob)\n",
        "    top_pred_prob = top_pred_prob.squeeze(0).cpu().numpy()\n",
        "    top_pred_label = top_pred_label.squeeze(0).cpu().numpy()\n",
        "    c = np.random.choice(top_pred_label, size=1, p=top_pred_prob)\n",
        "    return c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-02-18T03:31:48.860330Z",
          "start_time": "2018-02-18T03:31:48.772317Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generate text is: 天青色等烟雨 片帆天际波中象璧 不似到仙林何在 新春山月低心出 波透兔中\n"
          ]
        }
      ],
      "source": [
        "Begin = 'Azure color and other rains'\n",
        "text_len = 30\n",
        "\n",
        "model = model.eval()\n",
        "samples = [convert.word_to_int(c) for c in begin]\n",
        "input_txt = torch.LongTensor(samples)[None]\n",
        "if use_gpu:\n",
        "    input_txt = input_txt.cuda()\n",
        "input_txt = Variable(input_txt)\n",
        "_, init_state = model(input_txt)\n",
        "result = samples\n",
        "model_input = input_txt[:, -1][:, None]\n",
        "for i in range(text_len):\n",
        "    out, init_state = model(model_input, init_state)\n",
        "    pred = pick_top_n(out.data)\n",
        "    model_input = Variable(torch.LongTensor(pred))[None]\n",
        "    if use_gpu:\n",
        "        model_input = model_input.cuda()\n",
        "    result.append(pred[0])\n",
        "text = convert.arr_to_text(result)\n",
        "print('Generate text is: {}'.format(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Finally, you can see that the generated text has already thought of a paragraph.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}