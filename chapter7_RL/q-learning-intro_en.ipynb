{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "# Q Learning 介绍\n",
        "In enhanced learning, there is a well-known algorithm called q-learning. Let's start with the principle and then talk about q-learning through a simple small example.\n",
        "\n",
        "## q-learning principle\n",
        "We use a simple example to import q-learning. Suppose a house has 5 rooms and some rooms are connected. We want to be able to get out of this room.\n",
        "\n",
        "![](https://ws2.sinaimg.cn/large/006tNc79ly1fn70q0n91lj30h40a8aaf.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Then we can simplify it into the form of some nodes and graphs. Each room acts as a node. The two rooms are connected by a door. Just connect a line between the two nodes to get the picture below.\n",
        "\n",
        "![](https://ws4.sinaimg.cn/large/006tNc79ly1fn70r6c6koj30h60b2gm0.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "In order to simulate the whole process, we placed an agent in any room, hoping it can get out of the room, which means that it can go to node 5. In order to let the agent know that node 5 is the target room, we need to set some rewards. For each side, we associate a bonus value: the bonus value of the side directly connected to the target room is set to 100, and the other edges can be set. 0, note that room 5 has an arrow pointing to itself, the bonus value is also set to 100, and the other directly points to room 5 is also set to 100, so when the agent reaches room 5, he will choose one In Room 5, this is also called the absorption target, and the effect is as follows\n",
        "\n",
        "![](https://ws4.sinaimg.cn/large/006tNc79ly1fn71gf4idrj30c207u74i.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Think about the fact that the agent can keep learning. Every time we put it in one of the rooms, then it can continue to explore and walk to Room 5 according to the reward value, that is, out of the house. For example, now that the agent is in Room 2, we hope that it can continue to explore and go to Room 5.\n",
        "\n",
        "### Status and Action\n",
        "There are two important concepts in q-learning, one is state, the other is action, we call each room a state, and the agent moves from one room to another called an action, corresponding to the above The graph is that each node is a state, and each arrow is an action. If the agent is in state 4, slave state 4 can choose to go to state 0, or state 3 or state 5. If it reaches state 3, it can also choose to go to state 2 or state 1 or state 4.\n",
        "\n",
        "We can create a reward table based on the rewards of status and actions. Use -1 to indicate that there is no edge between the corresponding nodes, and the edge rewards that do not reach the end point are counted as 0, as follows\n",
        "\n",
        "![](https://ws2.sinaimg.cn/large/006tNc79ly1fn71o8jlinj307t055wek.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Similarly, we can let the agent continuously learn the knowledge in the environment through interaction with the environment, let the agent estimate the possible benefits of each action according to each state. This matrix is called Q table, and each row represents state. Each column represents a different action. For a situation with unknown state, we can randomly let the agent start from any position, and then explore the new environment to get all the states as possible. At first, the agent didn't know anything about the environment, so the values were all initialized to 0, as follows\n",
        "\n",
        "![](https://ws2.sinaimg.cn/large/006tNc79ly1fn71t3h3wnj306u053jrf.jpg)\n",
        "\n",
        "Our agents update the results in the Q table by continually learning, and finally make decisions based on the values in the Q table.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "### Q-learning algorithm\n",
        "With the rewards table and the Q table, we need to know how the agent updates the Q table by learning so that we can finally make decisions based on the Q table. This time we need to talk about the Q-learning algorithm.\n",
        "\n",
        "The algorithm of Q-learning is particularly simple, and the state transition formula is as follows\n",
        "\n",
        "$$Q(s, a) = R(s, a) + \\gamma \\mathop{max}_{\\tilde{a}}\\{ Q(\\tilde{s}, \\tilde{a}) \\}$$\n",
        "\n",
        "Where s, a represents the current state and action, $\\tilde{s}, \\tilde{a}$ respectively represent the next state after s takes the action of a and the action corresponds to all actions, the parameter $\\gamma$ is A constant, $0 \\leq \\gamma \\le 1 $ represents a degree of attenuation of future rewards, which is a metaphor for a person's vision of the future.\n",
        "\n",
        "解释一下就是智能体通过经验进行自主学习，不断从一个状态转移到另外一个状态进行探索，并在这个过程中不断更新 Q 表，直到到达目标位置，Q 表就像智能体的大脑，更新越多就越强。我们称智能体的每一次探索为 episode，每个 episode 都表示智能体从任意初始状态到达目标状态，当智能体到达一个目标状态，那么当前的 episode 结束，进入下一个 episode。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "The entire algorithm flow of q-learning is given below.\n",
        "- step1 given parameters $\\gamma$ and reward matrix R\n",
        "- step2 order Q:= 0\n",
        "- step3 For each episode:\n",
        "- 3.1 randomly select an initial state s\n",
        "- 3.2 If the target status is not reached, perform the following steps\n",
        "- (1) Select one of all possible actions in the current state s a\n",
        "- (2) Using the selected behavior a, get the next state $\\tilde{s}$\n",
        "- (3) Calculate Q(s, a) according to the previous transfer formula\n",
        "- (4) Let $s: = \\tilde{s}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "### Single step demo\n",
        "To better understand q-learning, we can exemplify one of them.\n",
        "\n",
        "First select $\\gamma = 0.8$, the initial state is 1, Q initializes to zero matrix\n",
        "\n",
        "![](https://ws2.sinaimg.cn/large/006tNc79ly1fn71t3h3wnj306u053jrf.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "![](https://ws2.sinaimg.cn/large/006tNc79ly1fn71o8jlinj307t055wek.jpg)\n",
        "\n",
        "Because it is state 1, we observe the second row of the R matrix. Negative numbers indicate illegal behavior. There are only two possibilities for the following state. Go to state 3 or go to state 5. Randomly, we can choose to go to state 5.\n",
        "\n",
        "What happens when we get to state 5? Looking at line 6 of the R matrix, you can see that it corresponds to three possible actions: go to state 1, 4 or 5, according to the above transfer formula, we have\n",
        "\n",
        "$$Q(1, 5) = R(1, 5) + 0.8 * max\\{Q(5, 1), Q(5, 4), Q(5, 5)\\} = 100 + 0.8 * max\\{0, 0, 0\\} = 100$$\n",
        "\n",
        "So now the Q matrix has been updated and changed.\n",
        "\n",
        "![](https://ws2.sinaimg.cn/large/006tNc79ly1fn8182u6xlj306y04mmx6.jpg)\n",
        "\n",
        "Now our state changes from 1 to 5, because 5 is the final target state, so an episode is completed and goes to the next episode.\n",
        "\n",
        "In the next episode, we randomly select an initial state and update the Q matrix. After a lot of episodes, the matrix Q approaches convergence, then our agent learns the optimal path from any state to the target state.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "From the above principle, we know the most important state transition formula of q-learning. This formula is also called Bellman Equation. Through this formula, we can continuously update the Q matrix and finally get a convergent Q matrix.\n",
        "\n",
        "Below we use code to implement this process\n",
        "\n",
        "We define a simple labyrinth process, that is\n",
        "\n",
        "![](https://ws1.sinaimg.cn/large/006tNc79ly1fn82ja4dkwj308d08d3yj.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "The initial position is randomly at state 0, state 1 and state 2, and then the agent is expected to go to state 3 to get the treasure. The feasible course of action above has been marked with an arrow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "The following defines the reward matrix. There are 4 rows and 5 columns in total. Each row represents the state of state 0 to state 3, and each column represents five states: up, down, left, right, and still. The 0 in the reward matrix indicates the infeasible route. For example, the first line, up and left are not feasible routes, all are represented by 0, going down will go to the trap, so use -10 for reward, right to go and still give -1 reward Because neither the trap was triggered nor the treasure was reached, but the process wasted time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "reward = np.array([[0, -10, 0, -1, -1],\n",
        "                   [0, 10, -1, 0, -1],\n",
        "                   [-1, 0, 0, 10, -10],\n",
        "                   [-1, 0, -10, 0, 10]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Next define a q matrix initialized to 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "q_matrix = np.zeros((4, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Then define a transition matrix, that is, a state that arrives from a state after taking a feasible action, because the states and actions here are limited, so we can save them, for example, the first row represents state 0, up. And left is not a feasible route, so the value of -1 means that it goes down to state 2, so the second value is 2, and the right direction reaches state 1, so the fourth value is 1. Keep it different or at state 0, so the last one is labeled 0, and the other lines are similar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "transition_matrix = np.array([[-1, 2, -1, 1, 0],\n",
        "                              [-1, 3, 0, -1, 1],\n",
        "                              [0, -1, -1, 3, 2],\n",
        "                              [1, -1, 2, -1, 3]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Finally define the effective actions for each state, such as the effective actions of state 0 are lower, right, and stationary, corresponding to 1, 3, and 4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "valid_actions = np.array([[1, 3, 4],\n",
        "                          [1, 2, 4],\n",
        "                          [0, 3, 4],\n",
        "                          [0, 2, 4]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define gamma in bellman equation\n",
        "gamma = 0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Finally, let the agent interact with the environment, and constantly use the bellman equation to update the q matrix. We run 10 episodes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode: 0, q matrix: \n",
            "[[  0.   0.   0.  -1.  -1.]\n",
            " [  0.  10.  -1.   0.  -1.]\n",
            " [  0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.]]\n",
            "\n",
            "episode: 1, q matrix: \n",
            "[[  0.   0.   0.  -1.  -1.]\n",
            " [  0.  10.  -1.   0.  -1.]\n",
            " [  0.   0.   0.  10.   0.]\n",
            " [  0.   0.   0.   0.   0.]]\n",
            "\n",
            "episode: 2, q matrix: \n",
            "[[  0.   -2.    0.    7.    4.6]\n",
            " [  0.   10.    4.6   0.    7. ]\n",
            " [ -1.8   0.    0.   10.   -2. ]\n",
            " [  0.    0.    0.    0.    0. ]]\n",
            "\n",
            "episode: 3, q matrix: \n",
            "[[  0.   -2.    0.    7.    4.6]\n",
            " [  0.   10.    4.6   0.    7. ]\n",
            " [  4.6   0.    0.   10.   -2. ]\n",
            " [  0.    0.    0.    0.    0. ]]\n",
            "\n",
            "episode: 4, q matrix: \n",
            "[[  0.   -2.    0.    7.    4.6]\n",
            " [  0.   10.    4.6   0.    7. ]\n",
            " [  4.6   0.    0.   10.   -2. ]\n",
            " [  0.    0.    0.    0.    0. ]]\n",
            "\n",
            "episode: 5, q matrix: \n",
            "[[  0.   -2.    0.    7.    4.6]\n",
            " [  0.   10.    4.6   0.    7. ]\n",
            " [  4.6   0.    0.   10.   -2. ]\n",
            " [  0.    0.    0.    0.    0. ]]\n",
            "\n",
            "episode: 6, q matrix: \n",
            "[[  0.   -2.    0.    7.    4.6]\n",
            " [  0.   10.    4.6   0.    7. ]\n",
            " [  4.6   0.    0.   10.   -2. ]\n",
            " [  0.    0.    0.    0.    0. ]]\n",
            "\n",
            "episode: 7, q matrix: \n",
            "[[  0.   -2.    0.    7.    4.6]\n",
            " [  0.   10.    4.6   0.    7. ]\n",
            " [  4.6   0.    0.   10.   -2. ]\n",
            " [  0.    0.    0.    0.    0. ]]\n",
            "\n",
            "episode: 8, q matrix: \n",
            "[[  0.   -2.    0.    7.    4.6]\n",
            " [  0.   10.    4.6   0.    7. ]\n",
            " [  4.6   0.    0.   10.   -2. ]\n",
            " [  0.    0.    0.    0.    0. ]]\n",
            "\n",
            "episode: 9, q matrix: \n",
            "[[  0.   -2.    0.    7.    4.6]\n",
            " [  0.   10.    4.6   0.    7. ]\n",
            " [  4.6   0.    0.   10.   -2. ]\n",
            " [  0.    0.    0.    0.    0. ]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "Start_state = np.random.choice([0, 1, 2], size=1)[0] # Random initial starting point\n",
        "    current_state = start_state\n",
        "While current_state != 3: # determine whether the end point is reached\n",
        "Action = random.choice(valid_actions[current_state]) # greedy randomly selects the active action in the current state\n",
        "Next_state = transition_matrix[current_state][action] # Get the next state by the selected action\n",
        "        future_rewards = []\n",
        "        for action_nxt in valid_actions[next_state]:\n",
        "Future_rewards.append(q_matrix[next_state][action_nxt]) # Get rewards for all possible actions in the next state\n",
        "        q_state = reward[current_state][action] + gamma * max(future_rewards) # bellman equation\n",
        "Q_matrix[current_state][action] = q_state # update q matrix\n",
        "Current_state = next_state # turns the next state into the current state\n",
        "        \n",
        "    print('episode: {}, q matrix: \\n{}'.format(i, q_matrix))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "It can be seen that after the first episode, the agent learns to go down in state 2 to get rewards. After learning continuously, after 10 episodes, the agent knows that in state 0, it can go to the right. Get rewards, go down in state 1 to get rewards, go right in state 3 to get rewards, so that in this environment any state agent can know how to get to the treasure location as quickly as possible\n",
        "\n",
        "From the above example we simply demonstrated q-learning, we can see that it is very troublesome to build the whole environment, so we can help us build a learning environment through some third-party libraries, the most famous of which is open- Ai's gym module, we will introduce gym in the next chapter.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}