{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "# 反向传播算法\n",
        "\n",
        "In the previous section, we introduced three models. The basic process of the whole process is to define the model, read in the data, give the loss function $f$, and update the parameters by the gradient descent method. PyTorch provides a very simple automatic derivation to help us solve the derivative. For simpler models, we can also manually determine the gradient of the parameters, but for very complex models, such as a 100-layer network, how can we effectively manually Find this gradient? Here we need to introduce a back propagation algorithm. The essence of automatic derivation is a back propagation algorithm.\n",
        "\n",
        "The backpropagation algorithm is an algorithm for effectively solving the gradient. It is essentially the application of a chained derivation rule. However, this simple and obvious method was invented nearly 30 years after Roseblatt proposed the perceptron algorithm. Popular, Bengio said: \"A lot of seemingly obvious ideas become apparent only afterwards.\"\n",
        "\n",
        "Let's take a closer look at what is a backpropagation algorithm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "## Chain Law\n",
        "\n",
        "First, let's briefly introduce the chain rule and consider a simple function, such as\n",
        "$$f(x, y, z) = (x + y)z$$\n",
        "\n",
        "We can of course directly find the differential of this function, but here we have to use the chain rule,\n",
        "$$q=x+y$$\n",
        "\n",
        "Then\n",
        "\n",
        "$$f = qz$$\n",
        "\n",
        "For these two equations, we can find their differentials separately.\n",
        "\n",
        "$$\\frac{\\partial f}{\\partial q} = z, \\frac{\\partial f}{\\partial z}=q$$\n",
        "\n",
        "At the same time $q$ is the sum of $x$ and $y$, so we can get\n",
        "\n",
        "$$\\frac{\\partial q}{x} = 1, \\frac{\\partial q}{y} = 1$$\n",
        "\n",
        "The problem we care about is\n",
        "\n",
        "$$\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z}$$\n",
        "\n",
        "The chain rule tells us how to calculate their value\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial x}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial y}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial z} = q\n",
        "$$\n",
        "\n",
        "Through chain-based rules, we know that if we need to derive the elements, we can multiply them one by one and multiply the results. This is the core of the chain rule and the core of the back propagation algorithm. More about The algorithm of the chain rule can access this [document] (https:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "## Backpropagation Algorithm\n",
        "\n",
        "Understand the chain rule, we can start to introduce the back propagation algorithm. In essence, the back propagation algorithm is only an application of the chain rule. We still use the same example $q=x y, f=qz$, which can be expressed by calculating the graph.\n",
        "\n",
        "![](https://ws1.sinaimg.cn/large/006tNc79ly1fmiozcinyzj30c806vglk.jpg)\n",
        "\n",
        "The green number above indicates its value, and the red number below indicates the gradient obtained. We can look at the implementation of the backpropagation algorithm step by step. First from the end, the gradient is of course 1, then calculated\n",
        "\n",
        "$$\\frac{\\partial f}{\\partial q} = z = -4,\\ \\frac{\\partial f}{\\partial z} = q = 3$$\n",
        "\n",
        "Then we calculate\n",
        "$$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x} = -4 \\times 1 = -4,\\ \\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial y} = -4 \\times 1 = -4$$\n",
        "\n",
        "This step by step we find $\\nabla f(x, y, z)$.\n",
        "\n",
        "Intuitively, the backpropagation algorithm is an elegant local process. Each derivation is only a derivative of the current operation. Solving the parameters of each layer of the network is based on the chain rule to find the previous result and iterate to this layer. , so this is a communication process\n",
        "\n",
        "### Sigmoid function example\n",
        "\n",
        "Below we use the Sigmoid function to demonstrate how the backpropagation process works on a complex function.\n",
        "\n",
        "$$\n",
        "f(w, x) = \\frac{1}{1+e^{-(w_0 x_0 + w_1 x_1 + w_2)}}\n",
        "$$\n",
        "\n",
        "We need to solve\n",
        "$$\\frac{\\partial f}{\\partial w_0}, \\frac{\\partial f}{\\partial w_1}, \\frac{\\partial f}{\\partial w_2}$$\n",
        "\n",
        "First we abstract this function into a computational graph, ie\n",
        "$$\n",
        "   f(x) = \\frac{1}{x} \\\\\n",
        "   f_c(x) = 1 + x \\\\\n",
        "   f_e(x) = e^x \\\\\n",
        "   f_w(x) = -(w_0 x_0 + w_1 x_1 + w_2)\n",
        "$$\n",
        "\n",
        "So we can draw the following calculation diagram\n",
        "\n",
        "![](https://ws1.sinaimg.cn/large/006tNc79ly1fmip1va5qjj30lb08e0t0.jpg)\n",
        "\n",
        "Similarly, the green number above represents the value, the red number below indicates the gradient, and we calculate the gradient of each parameter from the back to the front. First, the final gradient is 1, and then pass the $\\frac{1}{x}$ function. The gradient of this function is $-\\frac{1}{x^2}$, and the gradient before the previous propagation is $1 \\times -\\frac{1}{1.37^2} = -0.53$, then $1$ this operation, the gradient is unchanged, then the $e^x$ operation, its gradient is $-0.53 \\times E^{-1} = -0.2$, so that the gradient of each parameter can be obtained by continuously propagating backwards.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mx",
      "language": "python",
      "name": "mx"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}