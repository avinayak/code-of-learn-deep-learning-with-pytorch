{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "# 自动求导\n",
        "In this course we will learn about the automatic derivation mechanism in PyTorch. Auto-derivation is a very important feature in PyTorch, which allows us to avoid manually calculating very complex derivatives, which can greatly reduce the time we build the model. It is also a feature that its predecessor, Torch, does not have. Let's take a look at the unique charm of PyTorch's automatic derivation and explore more uses for auto-derivation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "## Simple situation automatic derivation\n",
        "Below we show some simple cases of automatic derivation, \"simple\" is reflected in the calculation results are scalar, that is, a number, we automatically deduct this scalar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variable containing:\n",
            " 19\n",
            "[torch.FloatTensor of size 1]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "x = Variable(torch.Tensor([2]), requires_grad=True)\n",
        "y = x + 2\n",
        "z = y ** 2 + 3\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Through the above column operations, we get the final result out from x, we can represent it as a mathematical formula\n",
        "\n",
        "$$\n",
        "z = (x + 2)^2 + 3\n",
        "$$\n",
        "\n",
        "Then the result of our derivation from z to x is\n",
        "\n",
        "$$\n",
        "\\frac{\\partial z}{\\partial x} = 2 (x + 2) = 2 (2 + 2) = 8\n",
        "$$\n",
        "If you are unfamiliar with the guide, you can check out the following [URL for review] (https:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variable containing:\n",
            " 8\n",
            "[torch.FloatTensor of size 1]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Using automatic derivation\n",
        "z.backward()\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "For a simple example like this, we verified the automatic derivation and found that it is very convenient to use automatic derivation. If it's a more complicated example, then manual derivation can be very troublesome, so the auto-derivation mechanism can help us save the troublesome mathematics. Let's look at a more complicated example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        
      },
      "outputs": [],
      "source": [
        "x = Variable(torch.randn(10, 20), requires_grad=True)\n",
        "y = Variable(torch.randn(10, 5), requires_grad=True)\n",
        "w = Variable(torch.randn(20, 5), requires_grad=True)\n",
        "\n",
        "Out = torch.mean(y - torch.matmul(x, w)) # torch.matmul is doing matrix multiplication\n",
        "out.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "If you are unfamiliar with matrix multiplication, you can check out the [URL for review] below (https:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variable containing:\n",
            "\n",
            "Columns 0 to 9 \n",
            "-0.0600 -0.0242 -0.0514  0.0882  0.0056 -0.0400 -0.0300 -0.0052 -0.0289 -0.0172\n",
            "-0.0600 -0.0242 -0.0514  0.0882  0.0056 -0.0400 -0.0300 -0.0052 -0.0289 -0.0172\n",
            "-0.0600 -0.0242 -0.0514  0.0882  0.0056 -0.0400 -0.0300 -0.0052 -0.0289 -0.0172\n",
            "-0.0600 -0.0242 -0.0514  0.0882  0.0056 -0.0400 -0.0300 -0.0052 -0.0289 -0.0172\n",
            "-0.0600 -0.0242 -0.0514  0.0882  0.0056 -0.0400 -0.0300 -0.0052 -0.0289 -0.0172\n",
            "-0.0600 -0.0242 -0.0514  0.0882  0.0056 -0.0400 -0.0300 -0.0052 -0.0289 -0.0172\n",
            "-0.0600 -0.0242 -0.0514  0.0882  0.0056 -0.0400 -0.0300 -0.0052 -0.0289 -0.0172\n",
            "-0.0600 -0.0242 -0.0514  0.0882  0.0056 -0.0400 -0.0300 -0.0052 -0.0289 -0.0172\n",
            "-0.0600 -0.0242 -0.0514  0.0882  0.0056 -0.0400 -0.0300 -0.0052 -0.0289 -0.0172\n",
            "-0.0600 -0.0242 -0.0514  0.0882  0.0056 -0.0400 -0.0300 -0.0052 -0.0289 -0.0172\n",
            "\n",
            "Columns 10 to 19 \n",
            "-0.0372  0.0144 -0.1074 -0.0363 -0.0189  0.0209  0.0618  0.0435 -0.0591  0.0103\n",
            "-0.0372  0.0144 -0.1074 -0.0363 -0.0189  0.0209  0.0618  0.0435 -0.0591  0.0103\n",
            "-0.0372  0.0144 -0.1074 -0.0363 -0.0189  0.0209  0.0618  0.0435 -0.0591  0.0103\n",
            "-0.0372  0.0144 -0.1074 -0.0363 -0.0189  0.0209  0.0618  0.0435 -0.0591  0.0103\n",
            "-0.0372  0.0144 -0.1074 -0.0363 -0.0189  0.0209  0.0618  0.0435 -0.0591  0.0103\n",
            "-0.0372  0.0144 -0.1074 -0.0363 -0.0189  0.0209  0.0618  0.0435 -0.0591  0.0103\n",
            "-0.0372  0.0144 -0.1074 -0.0363 -0.0189  0.0209  0.0618  0.0435 -0.0591  0.0103\n",
            "-0.0372  0.0144 -0.1074 -0.0363 -0.0189  0.0209  0.0618  0.0435 -0.0591  0.0103\n",
            "-0.0372  0.0144 -0.1074 -0.0363 -0.0189  0.0209  0.0618  0.0435 -0.0591  0.0103\n",
            "-0.0372  0.0144 -0.1074 -0.0363 -0.0189  0.0209  0.0618  0.0435 -0.0591  0.0103\n",
            "[torch.FloatTensor of size 10x20]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get the gradient of x\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variable containing:\n",
            "1.00000e-02 *\n",
            "  2.0000  2.0000  2.0000  2.0000  2.0000\n",
            "  2.0000  2.0000  2.0000  2.0000  2.0000\n",
            "  2.0000  2.0000  2.0000  2.0000  2.0000\n",
            "  2.0000  2.0000  2.0000  2.0000  2.0000\n",
            "  2.0000  2.0000  2.0000  2.0000  2.0000\n",
            "  2.0000  2.0000  2.0000  2.0000  2.0000\n",
            "  2.0000  2.0000  2.0000  2.0000  2.0000\n",
            "  2.0000  2.0000  2.0000  2.0000  2.0000\n",
            "  2.0000  2.0000  2.0000  2.0000  2.0000\n",
            "  2.0000  2.0000  2.0000  2.0000  2.0000\n",
            "[torch.FloatTensor of size 10x5]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get the gradient of y\n",
        "print(y.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variable containing:\n",
            " 0.1342  0.1342  0.1342  0.1342  0.1342\n",
            " 0.0507  0.0507  0.0507  0.0507  0.0507\n",
            " 0.0328  0.0328  0.0328  0.0328  0.0328\n",
            "-0.0086 -0.0086 -0.0086 -0.0086 -0.0086\n",
            " 0.0734  0.0734  0.0734  0.0734  0.0734\n",
            "-0.0042 -0.0042 -0.0042 -0.0042 -0.0042\n",
            " 0.0078  0.0078  0.0078  0.0078  0.0078\n",
            "-0.0769 -0.0769 -0.0769 -0.0769 -0.0769\n",
            " 0.0672  0.0672  0.0672  0.0672  0.0672\n",
            " 0.1614  0.1614  0.1614  0.1614  0.1614\n",
            "-0.0042 -0.0042 -0.0042 -0.0042 -0.0042\n",
            "-0.0970 -0.0970 -0.0970 -0.0970 -0.0970\n",
            "-0.0364 -0.0364 -0.0364 -0.0364 -0.0364\n",
            "-0.0419 -0.0419 -0.0419 -0.0419 -0.0419\n",
            " 0.0134  0.0134  0.0134  0.0134  0.0134\n",
            "-0.0251 -0.0251 -0.0251 -0.0251 -0.0251\n",
            " 0.0586  0.0586  0.0586  0.0586  0.0586\n",
            "-0.0050 -0.0050 -0.0050 -0.0050 -0.0050\n",
            " 0.1125  0.1125  0.1125  0.1125  0.1125\n",
            "-0.0096 -0.0096 -0.0096 -0.0096 -0.0096\n",
            "[torch.FloatTensor of size 20x5]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get the gradient of w\n",
        "print(w.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "The above mathematical formula is more complicated. After matrix multiplication, the corresponding elements of the two matrices are multiplied, and then all the elements are averaged. Interested students can manually calculate the gradient. Using PyTorch's automatic derivation, we can easily get x. The derivatives of y and w, because deep learning is full of a large number of matrix operations, so we have no way to manually find these derivatives, with automatic derivation can easily solve the problem of network updates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "## Automated Derivation of Complex Situations\n",
        "Above we show the automatic derivation in simple cases, which are all automatic derivation of scalars. You may have a question, how to automatically derive a vector or matrix? Interested students can try it first. Below we will introduce the automatic derivation mechanism for multidimensional arrays.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variable containing:\n",
            " 2  3\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n",
            "Variable containing:\n",
            " 0  0\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "m = Variable(torch.FloatTensor([[2, 3]]), requires_grad=True) #Build a 1 x 2 matrix\n",
        "n = Variable(torch.zeros(1, 2)) #Build a 0 matrix of the same size\n",
        "print(m)\n",
        "print(n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variable containing:\n",
            "  4  27\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate the value of the new n by the value in m\n",
        "n[0, 0] = m[0, 0] ** 2\n",
        "n[0, 1] = m[0, 1] ** 3\n",
        "print(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Write the above formula into a mathematical formula, you can get\n",
        "$$\n",
        "n = (n_0,\\ n_1) = (m_0^2,\\ m_1^3) = (2^2,\\ 3^3) \n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Below we directly propagate n backproper, that is, the derivative of n to m.\n",
        "\n",
        "At this time we need to define the definition of this derivative, that is, how to define\n",
        "\n",
        "$$\n",
        "\\frac{\\partial n}{\\partial m} = \\frac{\\partial (n_0,\\ n_1)}{\\partial (m_0,\\ m_1)}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "In PyTorch, if you want to call auto-derivation, you need to pass a parameter to `backward()`, which has the same shape as n, such as $(w_0,\\ w_1)$, then the result of auto-derivation. Is:\n",
        "$$\n",
        "\\frac{\\partial n}{\\partial m_0} = w_0 \\frac{\\partial n_0}{\\partial m_0} + w_1 \\frac{\\partial n_1}{\\partial m_0}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial n}{\\partial m_1} = w_0 \\frac{\\partial n_0}{\\partial m_1} + w_1 \\frac{\\partial n_1}{\\partial m_1}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        
      },
      "outputs": [],
      "source": [
        "N.backward(torch.ones_like(n)) # takes (w0, w1) as (1, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variable containing:\n",
            "  4  27\n",
            "[torch.FloatTensor of size 1x2]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(m.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "By automatically deriving we got the gradients 4 and 27, we can check it out\n",
        "$$\n",
        "\\frac{\\partial n}{\\partial m_0} = w_0 \\frac{\\partial n_0}{\\partial m_0} + w_1 \\frac{\\partial n_1}{\\partial m_0} = 2 m_0 + 0 = 2 \\times 2 = 4\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial n}{\\partial m_1} = w_0 \\frac{\\partial n_0}{\\partial m_1} + w_1 \\frac{\\partial n_1}{\\partial m_1} = 0 + 3 m_1^2 = 3 \\times 3^2 = 27\n",
        "$$\n",
        "By checking we can get the same result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "## Multiple automatic derivation\n",
        "By calling backward we can do an automatic derivation. If we call backward again, we will find that the program reports an error and there is no way to do it again. This is because PyTorch defaults to an automatic derivation, the calculation graph is discarded, so two automatic derivation needs to manually set a thing, we use the following small example to illustrate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variable containing:\n",
            " 18\n",
            "[torch.FloatTensor of size 1]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "x = Variable(torch.FloatTensor([3]), requires_grad=True)\n",
        "y = x * 2 + x ** 2 + 3\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        
      },
      "outputs": [],
      "source": [
        "Y.backward(retain_graph=True) # Set retain_graph to True to keep the calculation graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variable containing:\n",
            " 8\n",
            "[torch.FloatTensor of size 1]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "Y.backward() # Do another automatic derivation, this time does not retain the calculation graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variable containing:\n",
            " 16\n",
            "[torch.FloatTensor of size 1]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "It can be seen that the gradient of x becomes 16, because there are two automatic derivations, so the first gradient 8 and the second gradient 8 add up to a result of 16.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "**Little exercises**\n",
        "\n",
        "definition\n",
        "\n",
        "$$\n",
        "x = \n",
        "\\left[\n",
        "\\begin{matrix}\n",
        "x_0 \\\\\n",
        "x_1\n",
        "\\end{matrix}\n",
        "\\right] = \n",
        "\\left[\n",
        "\\begin{matrix}\n",
        "2 \\\\\n",
        "3\n",
        "\\end{matrix}\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "$$\n",
        "k = (k_0,\\ k_1) = (x_0^2 + 3 x_1,\\ 2 x_0 + x_1^2)\n",
        "$$\n",
        "\n",
        "We hope to get\n",
        "\n",
        "$$\n",
        "j = \\left[\n",
        "\\begin{matrix}\n",
        "\\frac{\\partial k_0}{\\partial x_0} & \\frac{\\partial k_0}{\\partial x_1} \\\\\n",
        "\\frac{\\partial k_1}{\\partial x_0} & \\frac{\\partial k_1}{\\partial x_1}\n",
        "\\end{matrix}\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "Reference answer:\n",
        "\n",
        "$$\n",
        "\\left[\n",
        "\\begin{matrix}\n",
        "4 & 3 \\\\\n",
        "2 & 6 \\\\\n",
        "\\end{matrix}\n",
        "\\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "x = Variable(torch.FloatTensor([2, 3]), requires_grad=True)\n",
        "k = Variable(torch.zeros(2))\n",
        "\n",
        "k[0] = x[0] ** 2 + 3 * x[1]\n",
        "k[1] = x[1] ** 2 + 2 * x[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variable containing:\n",
            " 13\n",
            " 13\n",
            "[torch.FloatTensor of size 2]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        
      },
      "outputs": [],
      "source": [
        "j = torch.zeros(2, 2)\n",
        "\n",
        "k.backward(torch.FloatTensor([1, 0]), retain_graph=True)\n",
        "j[0] = x.grad.data\n",
        "\n",
        "X.grad.data.zero_() # Gradient obtained before returning to zero\n",
        "\n",
        "k.backward(torch.FloatTensor([0, 1]))\n",
        "j[1] = x.grad.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " 4  3\n",
            " 2  6\n",
            "[torch.FloatTensor of size 2x2]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(j)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "In the next lesson we will introduce two neural network programming methods, dynamic graph programming and static graph programming.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}