{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "# RNN 做图像分类\n",
        "前面我们讲了 RNN 特别适合做序列类型的数据，那么 RNN 能不能想 CNN 一样用来做图像分类呢？下面我们用 mnist 手写字体的例子来展示一下如何用 RNN 做图像分类，但是这种方法并不是主流，这里我们只是作为举例。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "For a handwritten font image, the size is 28 * 28, we can think of it as a sequence of length 28, each sequence has a feature of 28, which is\n",
        "\n",
        "![](https://ws4.sinaimg.cn/large/006tKfTcly1fmu7d0byfkj30n60djdg5.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "So we solved the problem of the input sequence, what about the output sequence? In fact, it's very simple. Although our output is a sequence, we only need to keep one of them as the output. In this case, it is best to keep the last result, because the last result has the information of all the previous sequences. Like below\n",
        "\n",
        "![](https://ws3.sinaimg.cn/large/006tKfTcly1fmu7fpqri0j30c407yjr8.jpg)\n",
        "\n",
        "Below we show directly through examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-26T08:01:44.502896Z",
          "start_time": "2017-12-26T08:01:44.062542Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import transforms as tfs\n",
        "from torchvision.datasets import MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-26T08:01:50.714439Z",
          "start_time": "2017-12-26T08:01:50.650872Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define data\n",
        "data_tf = tfs.Compose([\n",
        "    tfs.ToTensor(),\n",
        "tfs.Normalize([0.5], [0.5]) # normalization\n",
        "])\n",
        "\n",
        "train_set = MNIST('./data', train=True, transform=data_tf)\n",
        "test_set = MNIST('./data', train=False, transform=data_tf)\n",
        "\n",
        "train_data = DataLoader(train_set, 64, True, num_workers=4)\n",
        "test_data = DataLoader(test_set, 128, False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-26T08:01:51.165144Z",
          "start_time": "2017-12-26T08:01:51.115807Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define model\n",
        "class rnn_classify(nn.Module):\n",
        "    def __init__(self, in_feature=28, hidden_feature=100, num_class=10, num_layers=2):\n",
        "        super(rnn_classify, self).__init__()\n",
        "Self.rnn = nn.LSTM(in_feature, hidden_feature, num_layers) # Use two layers lstm\n",
        "Self.classifier = nn.Linear(hidden_feature, num_class) # Use the full join of the output of the last rnn to get the final classification result\n",
        "        \n",
        "    def forward(self, x):\n",
        "        '''\n",
        "The x size is (batch, 1, 28, 28), so we need to convert it to the input form of RNN, ie (28, batch, 28)\n",
        "        '''\n",
        "x = x.squeeze() # Remove 1 from (batch, 1, 28, 28) to (batch, 28, 28)\n",
        "x = x.permute(2, 0, 1) # Put the last dimension into the first dimension and become (28, batch, 28)\n",
        "Out, _ = self.rnn(x) # Using the default hidden state, the resulting out is (28, batch, hidden_feature)\n",
        "Out = out[-1, :, :] # Take the last one in the sequence, the size is (batch, hidden_feature)\n",
        "Out = self.classifier(out) # Get the classification result\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-26T08:01:51.252533Z",
          "start_time": "2017-12-26T08:01:51.244612Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "net = rnn_classify()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimzier = torch.optim.Adadelta(net.parameters(), 1e-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-26T08:03:36.739732Z",
          "start_time": "2017-12-26T08:01:51.607967Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0. Train Loss: 1.858605, Train Acc: 0.318347, Valid Loss: 1.147508, Valid Acc: 0.578125, Time 00:00:09\n",
            "Epoch 1. Train Loss: 0.503072, Train Acc: 0.848514, Valid Loss: 0.300552, Valid Acc: 0.912579, Time 00:00:09\n",
            "Epoch 2. Train Loss: 0.224762, Train Acc: 0.934785, Valid Loss: 0.176321, Valid Acc: 0.946499, Time 00:00:09\n",
            "Epoch 3. Train Loss: 0.157010, Train Acc: 0.953392, Valid Loss: 0.155280, Valid Acc: 0.954015, Time 00:00:09\n",
            "Epoch 4. Train Loss: 0.125926, Train Acc: 0.962137, Valid Loss: 0.105295, Valid Acc: 0.969640, Time 00:00:09\n",
            "Epoch 5. Train Loss: 0.104938, Train Acc: 0.968450, Valid Loss: 0.091477, Valid Acc: 0.972805, Time 00:00:10\n",
            "Epoch 6. Train Loss: 0.089124, Train Acc: 0.973481, Valid Loss: 0.104799, Valid Acc: 0.969343, Time 00:00:09\n",
            "Epoch 7. Train Loss: 0.077920, Train Acc: 0.976679, Valid Loss: 0.084242, Valid Acc: 0.976661, Time 00:00:10\n",
            "Epoch 8. Train Loss: 0.070259, Train Acc: 0.978795, Valid Loss: 0.078536, Valid Acc: 0.977749, Time 00:00:09\n",
            "Epoch 9. Train Loss: 0.063089, Train Acc: 0.981093, Valid Loss: 0.066984, Valid Acc: 0.980716, Time 00:00:09\n"
          ]
        }
      ],
      "source": [
        "#开始培训\n",
        "from utils import train\n",
        "train(net, train_data, test_data, 10, optimzier, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "It can be seen that training 10 times also achieved 98% accuracy on the simple mnist dataset, so RNN can also do simple image classification, but this is not his main battlefield. Speaking of a usage scenario of RNN, time series prediction.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}