{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "# LSTM 做词性预测\n",
        "Earlier we talked about word embedding and n-gram models for word prediction, but we haven't used RNN yet. In the last lesson, we will combine all the pre-requisites mentioned above to teach you how to use LSTM for part-of-speech prediction.\n",
        "\n",
        "##Model Introduction\n",
        "For a word, there will be different word-of-speech. Firstly, it can be judged according to the suffix of a word. For example, the suffix of -ly, the probability is an adverb. In addition, a same word can represent two different words. The part of speech, such as book, can represent both nouns and verbs, so in the end, what part of the word needs to be combined with the text before and after.\n",
        "\n",
        "According to this problem, we can use the lstm model to make predictions. First, for a word, we can think of it as a sequence. For example, apple is composed of five words of apple, which forms a sequence of 5, we can match these characters. Construct word embedding, then enter lstm, just like lstm does image classification, only the last output is used as the prediction result. The string of the whole word can form a memory characteristic, which helps us to better predict the part of speech.\n",
        "\n",
        "![](https://ws3.sinaimg.cn/large/006tKfTcgy1fmxi67w0f7j30ap05qq2u.jpg)\n",
        "\n",
        "Then we put the word and its first few words into a sequence, we can construct a new word embedding for these words, and finally the output is the word part of the word, that is, the word part of the word is classified according to the information of the previous words.\n",
        "\n",
        "Below we use the example to explain briefly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "We use the following simple training set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "training_data = [(\"The dog ate the apple\".split(),\n",
        "                  [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "                 (\"Everybody read that book\".split(), \n",
        "                  [\"NN\", \"V\", \"DET\", \"NN\"])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Next we need to encode the words and tags.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "word_to_idx = {}\n",
        "tag_to_idx = {}\n",
        "for context, tag in training_data:\n",
        "    for word in context:\n",
        "        if word.lower() not in word_to_idx:\n",
        "            word_to_idx[word.lower()] = len(word_to_idx)\n",
        "    for label in tag:\n",
        "        if label.lower() not in tag_to_idx:\n",
        "            tag_to_idx[label.lower()] = len(tag_to_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'apple': 3,\n",
              " 'ate': 2,\n",
              " 'book': 7,\n",
              " 'dog': 1,\n",
              " 'everybody': 4,\n",
              " 'read': 5,\n",
              " 'that': 6,\n",
              " 'the': 0}"
            ]
          },
          "execution_count": 4,
          "metadata": {
            
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'det': 0, 'nn': 1, 'v': 2}"
            ]
          },
          "execution_count": 5,
          "metadata": {
            
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tag_to_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Then we encode the letters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
        "char_to_idx = {}\n",
        "for i in range(len(alphabet)):\n",
        "    char_to_idx[alphabet[i]] = i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'a': 0,\n",
              " 'b': 1,\n",
              " 'c': 2,\n",
              " 'd': 3,\n",
              " 'e': 4,\n",
              " 'f': 5,\n",
              " 'g': 6,\n",
              " 'h': 7,\n",
              " 'i': 8,\n",
              " 'j': 9,\n",
              " 'k': 10,\n",
              " 'l': 11,\n",
              " 'm': 12,\n",
              " 'n': 13,\n",
              " 'o': 14,\n",
              " 'p': 15,\n",
              " 'q': 16,\n",
              " 'r': 17,\n",
              " 's': 18,\n",
              " 't': 19,\n",
              " 'u': 20,\n",
              " 'v': 21,\n",
              " 'w': 22,\n",
              " 'x': 23,\n",
              " 'y': 24,\n",
              " 'z': 25}"
            ]
          },
          "execution_count": 7,
          "metadata": {
            
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "char_to_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Then we can build the training data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "Def make_sequence(x, dic): # character encoding\n",
        "    idx = [dic[i.lower()] for i in x]\n",
        "    idx = torch.LongTensor(idx)\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\n",
              "  0\n",
              " 15\n",
              " 15\n",
              " 11\n",
              "  4\n",
              "[torch.LongTensor of size 5]"
            ]
          },
          "execution_count": 9,
          "metadata": {
            
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "make_sequence('apple', char_to_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Everybody', 'read', 'that', 'book']"
            ]
          },
          "execution_count": 10,
          "metadata": {
            
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_data[1][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\n",
              " 4\n",
              " 5\n",
              " 6\n",
              " 7\n",
              "[torch.LongTensor of size 4]"
            ]
          },
          "execution_count": 11,
          "metadata": {
            
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "make_sequence(training_data[1][0], word_to_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Construct a lstm model of a single character\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "class char_lstm(nn.Module):\n",
        "    def __init__(self, n_char, char_dim, char_hidden):\n",
        "        super(char_lstm, self).__init__()\n",
        "        \n",
        "        self.char_embed = nn.Embedding(n_char, char_dim)\n",
        "        self.lstm = nn.LSTM(char_dim, char_hidden)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.char_embed(x)\n",
        "        out, _ = self.lstm(x)\n",
        "        return out[-1] # (batch, hidden)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Constructing the lstm model of part of speech classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "class lstm_tagger(nn.Module):\n",
        "    def __init__(self, n_word, n_char, char_dim, word_dim, \n",
        "                 char_hidden, word_hidden, n_tag):\n",
        "        super(lstm_tagger, self).__init__()\n",
        "        self.word_embed = nn.Embedding(n_word, word_dim)\n",
        "        self.char_lstm = char_lstm(n_char, char_dim, char_hidden)\n",
        "        self.word_lstm = nn.LSTM(word_dim + char_hidden, word_hidden)\n",
        "        self.classify = nn.Linear(word_hidden, n_tag)\n",
        "        \n",
        "    def forward(self, x, word):\n",
        "        char = []\n",
        "For w in word: # lstm for each word\n",
        "            char_list = make_sequence(w, char_to_idx)\n",
        "Char_list = char_list.unsqueeze(1) # (seq, batch, feature) satisfies the lstm input condition\n",
        "            char_infor = self.char_lstm(Variable(char_list)) # (batch, char_hidden)\n",
        "            char.append(char_infor)\n",
        "        char = torch.stack(char, dim=0) # (seq, batch, feature)\n",
        "        \n",
        "        x = self.word_embed(x) # (batch, seq, word_dim)\n",
        "x = x.permute(1, 0, 2) # change order\n",
        "x = torch.cat((x, char), dim=2) # splicing the word embedding of each word along with the result of the character lstm output along the feature channel\n",
        "        x, _ = self.word_lstm(x)\n",
        "        \n",
        "        s, b, h = x.shape\n",
        "x = x.view(-1, h) # re reshape to classify the linear layer\n",
        "        out = self.classify(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "net = lstm_tagger(len(word_to_idx), len(char_to_idx), 10, 100, 50, 128, len(tag_to_idx))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 50, Loss: 0.86690\n",
            "Epoch: 100, Loss: 0.65471\n",
            "Epoch: 150, Loss: 0.45582\n",
            "Epoch: 200, Loss: 0.30351\n",
            "Epoch: 250, Loss: 0.20446\n",
            "Epoch: 300, Loss: 0.14376\n"
          ]
        }
      ],
      "source": [
        "#开始培训\n",
        "for e in range(300):\n",
        "    train_loss = 0\n",
        "    for word, tag in training_data:\n",
        "Word_list = make_sequence(word, word_to_idx).unsqueeze(0) # Add the first dimension batch\n",
        "        tag = make_sequence(tag, tag_to_idx)\n",
        "        word_list = Variable(word_list)\n",
        "        tag = Variable(tag)\n",
        "#向向传播\n",
        "        out = net(word_list, word)\n",
        "        loss = criterion(out, tag)\n",
        "        train_loss += loss.data[0]\n",
        "#反传播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if (e + 1) % 50 == 0:\n",
        "        print('Epoch: {}, Loss: {:.5f}'.format(e + 1, train_loss / len(training_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Finally, we can look at the predicted results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "net = net.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        
      },
      "outputs": [],
      "source": [
        "test_sent = 'Everybody ate the apple'\n",
        "test = make_sequence(test_sent.split(), word_to_idx).unsqueeze(0)\n",
        "out = net(Variable(test), test_sent.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variable containing:\n",
            "-1.2148  1.9048 -0.6570\n",
            "-0.9272 -0.4441  1.4009\n",
            " 1.6425 -0.7751 -1.1553\n",
            "-0.6121  1.6036 -1.1280\n",
            "[torch.FloatTensor of size 4x3]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'det': 0, 'nn': 1, 'v': 2}\n"
          ]
        }
      ],
      "source": [
        "print(tag_to_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Finally, you can get the above result, because the linear layer of the last layer does not use softmax, so the value is not like a probability, but the largest value of each row means that it belongs to the class, you can see that the first word 'Everybody' belongs to nn The second word 'ate' belongs to v, the third word 'the' belongs to det, and the fourth word 'apple' belongs to nn, so the prediction result obtained is correct.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}