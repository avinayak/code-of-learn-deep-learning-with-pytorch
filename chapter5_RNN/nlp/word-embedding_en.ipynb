{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "#词嵌入\n",
        "前面讲了循环神经网络做简单的图像分类问题和飞机流量时序预测，但是现在循环神经网络最火热的应用是自然语言处理，下面我们介绍一下自然语言处理中如果运用循环神经网络，首先我们介绍一下第一个概念，词嵌入。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "For image classification problems, we can use one-hot type to encode, for example, there are 5 categories, then the second category can be represented by (0, 1, 0, 0, 0), for classification problems, of course It's simple, but in natural language processing, because there are too many words, it won't work. For example, there are 10,000 different words, so using one-hot is not only inefficient, but also has no way to express the characteristics of the words. At this time, word embedding was introduced to express each word.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "The word vector simply uses a vector to represent a word, but the vector is not random, because it doesn't make any sense, so we need to have a specific vector for each word to represent them, and some words. The part of speech is similar, such as \"(love) like\" and \"like love\". For words with similar meanings, we need their vector representations to be similar. How to measure and define the similarity between vectors? Very simple, is to use the angle between the two vectors, the smaller the angle, the closer, so there is a complete definition.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Let's take an example. There are 4 paragraphs below.\n",
        "\n",
        "1. The cat likes playing wool.\n",
        "\n",
        "2. The kitty likes playing wool.\n",
        "\n",
        "3. The dog likes playing ball.\n",
        "\n",
        "4. The boy does not like playing ball or wool.\n",
        "\n",
        "There are 4 words in it, cat, kitty, dog and boy. Below we use a two-dimensional word vector (a, b) to represent each word, where a, b respectively represent a property of the word, such as a for whether you like to play the ball, b for whether you like to play with the yarn, the value The bigger the expression, the more like it is, then we can use values to define each word.\n",
        "\n",
        "For cat, we can define its word embedding as (-1, 4) because he doesn't like to play with the ball, likes to play with the wool, and can define kitty as (-2, 5), dog as (3, 2) and boy. For (-2, -3), then the four vectors are represented in the coordinate system, that is\n",
        "\n",
        "<img src=\"https://ws1.sinaimg.cn/large/006tNc79gy1fmwf2jxhbzj30g40b2my2.jpg\" width=\"350\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "As you can see, the above image shows the angle between different word embeddings. The angle between kitty and cat is relatively small, so they are more similar. The angle between dog and boy is very large, so they are Not similar.\n",
        "\n",
        "Let's see how to call the word vector in pytorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "## PyTorch implementation\n",
        "The word embedding in pytorch is very simple, just call `torch.nn.Embedding(m, n)`, m means the total number of words, n is the dimension of the word embedding, in fact the word embedding is equivalent to a large matrix. , each line of the matrix represents a word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define word embedding\n",
        "Embeds = nn.Embedding(2, 5) # 2 words, dimension 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "-1.3426  0.7316 -0.2437  0.4925 -0.0191\n",
              "-0.8326  0.3367  0.2135  0.5059  0.8326\n",
              "[torch.FloatTensor of size 2x5]"
            ]
          },
          "execution_count": 5,
          "metadata": {
            
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Get the word embedding matrix\n",
        "embeds.weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "We get the matrix embedded by the whole word through `weight`. Note that this matrix is a parameter that can be changed. It will be updated continuously in the training of the network, and the value of the word embedded can be directly modified. For example, we can read in a pre- Trained word embedding, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              " 1  1  1  1  1\n",
              " 1  1  1  1  1\n",
              "[torch.FloatTensor of size 2x5]"
            ]
          },
          "execution_count": 8,
          "metadata": {
            
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Directly modify the value of the word embedded\n",
        "embeds.weight.data = torch.ones(2, 5)\n",
        "embeds.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# access to the 50th word of the word vector\n",
        "embeds = nn.Embedding(100, 10)\n",
        "single_word_embed = embeds(Variable(torch.LongTensor([50])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Variable containing:\n",
              "-1.4954 -1.8475  0.2913 -0.9674 -2.1250 -0.5783 -0.6717  0.5638  0.7038  0.4437\n",
              "[torch.FloatTensor of size 1x10]"
            ]
          },
          "execution_count": 12,
          "metadata": {
            
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "single_word_embed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "You can see that if we want to access the word vector of one of the words, we can directly call the defined word embedding, but the input must pass in a Variable and the type is LongTensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Although we know how to define the similarity of word vectors, we still don't know how to get word embedding, because if a word is embedded in 100 dimensions, it is obviously impossible to assign values by humans, so in order to get the word vector, we need to introduce skip-gram. model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "## Skip-Gram Model\n",
        "The Skip Gram model is [Word2Vec] (https:\n",
        "\n",
        "## Model Structure\n",
        "The skip-gram model is very simple. We train a simple network in a piece of text. The task of this network is to predict the word by a word around a word, but what we actually have to do is train our word embedding.\n",
        "\n",
        "For example, if we give a word in a sentence, look at the words around it, and then randomly pick one, we want the network to output a probability value that tells us how far the word is from the word we choose. For example, the phrase 'A dog is playing with a ball', if the word we choose is 'ball', then 'playing' is closer to the word we choose than 'dog'.\n",
        "\n",
        "For a paragraph, we can select different words in order, then build training samples and label, such as\n",
        "\n",
        "![](https://ws2.sinaimg.cn/large/006tNc79gy1fmwlpfp3loj30hh0ah75l.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "For this example, we take a word and the words around it to form a training sample. For example, if the first word selected is 'the', then we take the two words as the training sample. This can also be called a Sliding window, for the first word, there is no word on the left side, so the training set is three words, then we select 'the' as input in the three words, and the other two words are his output, which constitutes For the two training samples, for example, select the word 'fox', then add the two words on the left, the two words on the right, a total of 5 words, and then select 'fox' as the input, then the output is the four around it. Words can form a total of 4 training samples. By this method, we can train the required word embedding.\n",
        "\n",
        "In the next lesson, we will talk about the use of words in the end.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}