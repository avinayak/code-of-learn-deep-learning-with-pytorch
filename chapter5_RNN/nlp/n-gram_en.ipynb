{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "# N-Gram Model\n",
        "上一节课，我们讲了词嵌入以及词嵌入是如何得到的，现在我们来讲讲词嵌入如何来训练语言模型，首先我们介绍一下 N-Gram 模型的原理和其要解决的问题。\n",
        "\n",
        "For a sentence, the order of the words is very important, so can we predict the next few words from the previous words, such as 'I lived in France for 10 years, I can speak _' In the middle, we can predict that the last word is French.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "对于一句话 T，其由 $w_1, w_2, \\cdots, w_n$ 这 n 个词构成，\n",
        "\n",
        "$$\n",
        "P(T) = P(w_1)P(w_2 | w_1)P(w_3 |w_2 w_1) \\cdots P(w_n |w_{n-1} w_{n-2}\\cdots w_2w_1)\n",
        "$$\n",
        "\n",
        "We can simplify this model again. For example, for a word, it does not need all the preceding words as conditional probabilities, that is, a word can only be related to several words in front of it. This is the Markov assumption.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "For the conditional probability here, the traditional method is to estimate the frequency of occurrence of each word in the corpus, and estimate the conditional probability according to Bayes' theorem. Here we can replace it with word embedding, and then use RNN for conditional probability. Computation, and then maximizing this conditional probability, not only modifies the word embedding, but also enables the model to predict one of the words based on the calculated conditional probability.\n",
        "\n",
        "Below we explain directly with the code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "CONTEXT_SIZE = 2 # Number of words based on\n",
        "EMBEDDING_DIM = 10 # Dimensions of the word vector\n",
        "# We use Shakespeare's poems\n",
        "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty's field,\n",
        "Thy youth's proud livery so gazed on now,\n",
        "Will be a totter'd weed of small worth held:\n",
        "Then being asked, where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days;\n",
        "To say, within thine own deep sunken eyes,\n",
        "Were an all-eating shame, and thriftless praise.\n",
        "How much more praise deserv'd thy beauty's use,\n",
        "If thou couldst answer 'This fair child of mine\n",
        "Shall sum my count, and make my old excuse,'\n",
        "Proving his beauty by succession thine!\n",
        "This were to be new made when thou art old,\n",
        "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "The `CONTEXT_SIZE` here means that we want to predict the word from the first few words. Here we use two words, `EMBEDDING_DIM` to indicate the dimension of the word embedding.\n",
        "\n",
        "Then we build a training set that facilitates the entire corpus, grouping the three words, the first two as input, and the last as the result of the prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        " trigram = [((test_sentence[i], test_sentence[i+1]), test_sentence[i+2]) \n",
        "            for i in range(len(test_sentence)-2)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "113"
            ]
          },
          "execution_count": 5,
          "metadata": {
            
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Total amount of data\n",
        "len(trigram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(('When', 'forty'), 'winters')"
            ]
          },
          "execution_count": 6,
          "metadata": {
            
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Take the first data and see\n",
        "trigram[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        
      },
      "outputs": [],
      "source": [
        "#Create a code for each word and number, and build word embedding accordingly\n",
        "Vocb = set(test_sentence) # Use set to remove duplicate elements\n",
        "word_to_idx = {word: i for i, word in enumerate(vocb)}\n",
        "idx_to_word = {word_to_idx[word]: word for word in word_to_idx}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\"'This\": 94,\n",
              " 'And': 71,\n",
              " 'How': 18,\n",
              " 'If': 49,\n",
              " 'Proving': 78,\n",
              " 'Shall': 48,\n",
              " 'Then': 33,\n",
              " 'This': 68,\n",
              " 'Thy': 75,\n",
              " 'To': 81,\n",
              " 'Were': 61,\n",
              " 'When': 14,\n",
              " 'Where': 95,\n",
              " 'Will': 27,\n",
              " 'a': 21,\n",
              " 'all': 53,\n",
              " 'all-eating': 3,\n",
              " 'an': 15,\n",
              " 'and': 23,\n",
              " 'answer': 80,\n",
              " 'art': 70,\n",
              " 'asked,': 69,\n",
              " 'be': 29,\n",
              " 'beauty': 16,\n",
              " \"beauty's\": 40,\n",
              " 'being': 79,\n",
              " 'besiege': 55,\n",
              " 'blood': 11,\n",
              " 'brow,': 1,\n",
              " 'by': 59,\n",
              " 'child': 8,\n",
              " 'cold.': 32,\n",
              " 'couldst': 26,\n",
              " 'count,': 77,\n",
              " 'days;': 43,\n",
              " 'deep': 62,\n",
              " \"deserv'd\": 41,\n",
              " 'dig': 64,\n",
              " \"excuse,'\": 86,\n",
              " 'eyes,': 84,\n",
              " 'fair': 56,\n",
              " \"feel'st\": 44,\n",
              " 'field,': 9,\n",
              " 'forty': 46,\n",
              " 'gazed': 93,\n",
              " 'held:': 12,\n",
              " 'his': 89,\n",
              " 'in': 45,\n",
              " 'it': 34,\n",
              " 'lies,': 57,\n",
              " 'livery': 28,\n",
              " 'lusty': 65,\n",
              " 'made': 54,\n",
              " 'make': 42,\n",
              " 'mine': 13,\n",
              " 'more': 83,\n",
              " 'much': 30,\n",
              " 'my': 50,\n",
              " 'new': 92,\n",
              " 'now,': 25,\n",
              " 'of': 47,\n",
              " 'old': 22,\n",
              " 'old,': 19,\n",
              " 'on': 74,\n",
              " 'own': 20,\n",
              " 'praise': 38,\n",
              " 'praise.': 96,\n",
              " 'proud': 5,\n",
              " 'say,': 63,\n",
              " 'see': 58,\n",
              " 'shall': 87,\n",
              " 'shame,': 90,\n",
              " 'small': 31,\n",
              " 'so': 67,\n",
              " 'succession': 36,\n",
              " 'sum': 10,\n",
              " 'sunken': 60,\n",
              " 'the': 73,\n",
              " 'thine': 24,\n",
              " 'thine!': 0,\n",
              " 'thou': 51,\n",
              " 'thriftless': 72,\n",
              " 'thy': 76,\n",
              " 'to': 85,\n",
              " \"totter'd\": 2,\n",
              " 'treasure': 17,\n",
              " 'trenches': 39,\n",
              " 'use,': 35,\n",
              " 'warm': 66,\n",
              " 'weed': 91,\n",
              " 'were': 82,\n",
              " 'when': 7,\n",
              " 'where': 37,\n",
              " 'winters': 88,\n",
              " 'within': 4,\n",
              " 'worth': 52,\n",
              " \"youth's\": 6}"
            ]
          },
          "execution_count": 13,
          "metadata": {
            
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_to_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "From the above you can see that each word corresponds to a number, and the words here are all different.\n",
        "\n",
        "Then we define the model. The input of the model is the first two words, and the output is the probability of predicting the word.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        
      },
      "outputs": [],
      "source": [
        "# Define model\n",
        "class n_gram(nn.Module):\n",
        "    def __init__(self, vocab_size, context_size=CONTEXT_SIZE, n_dim=EMBEDDING_DIM):\n",
        "        super(n_gram, self).__init__()\n",
        "        \n",
        "        self.embed = nn.Embedding(vocab_size, n_dim)\n",
        "        self.classify = nn.Sequential(\n",
        "            nn.Linear(context_size * n_dim, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, vocab_size)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "Voc_embed = self.embed(x) # get word embedding\n",
        "Voc_embed = voc_embed.view(1, -1) # Put two word vectors together\n",
        "        out = self.classify(voc_embed)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Finally, our output is a conditional probability, which is equivalent to a classification problem. We can use cross entropy to easily measure the error.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "net = n_gram(len(word_to_idx))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=1e-2, weight_decay=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 20, Loss: 0.088273\n",
            "epoch: 40, Loss: 0.065301\n",
            "epoch: 60, Loss: 0.057113\n",
            "epoch: 80, Loss: 0.052442\n",
            "epoch: 100, Loss: 0.049236\n"
          ]
        }
      ],
      "source": [
        "for e in range(100):\n",
        "    train_loss = 0\n",
        "For word, label in trigram: # using the first 100 as a training set\n",
        "Word = Variable(torch.LongTensor([word_to_idx[i] for i in word])) # Enter two words as input\n",
        "        label = Variable(torch.LongTensor([word_to_idx[label]]))\n",
        "#向向传播\n",
        "        out = net(word)\n",
        "        loss = criterion(out, label)\n",
        "        train_loss += loss.data[0]\n",
        "#反传播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if (e + 1) % 20 == 0:\n",
        "        print('epoch: {}, Loss: {:.6f}'.format(e + 1, train_loss / len(trigram)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Finally we can test the results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "net = net.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input: ('so', 'gazed')\n",
            "label: on\n",
            "\n",
            "real word is on, predicted word is on\n"
          ]
        }
      ],
      "source": [
        "# test the results\n",
        "word, label = trigram[19]\n",
        "print('input: {}'.format(word))\n",
        "print('label: {}'.format(label))\n",
        "print()\n",
        "word = Variable(torch.LongTensor([word_to_idx[i] for i in word]))\n",
        "out = net(word)\n",
        "pred_label_idx = out.max(1)[1].data[0]\n",
        "predict_word = idx_to_word[pred_label_idx]\n",
        "print('real word is {}, predicted word is {}'.format(label, predict_word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input: (\"'This\", 'fair')\n",
            "label: child\n",
            "\n",
            "real word is child, predicted word is child\n"
          ]
        }
      ],
      "source": [
        "word, label = trigram[75]\n",
        "print('input: {}'.format(word))\n",
        "print('label: {}'.format(label))\n",
        "print()\n",
        "word = Variable(torch.LongTensor([word_to_idx[i] for i in word]))\n",
        "out = net(word)\n",
        "pred_label_idx = out.max(1)[1].data[0]\n",
        "predict_word = idx_to_word[pred_label_idx]\n",
        "print('real word is {}, predicted word is {}'.format(label, predict_word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "It can be seen that the network can basically predict accuracy on the training set, but there are too few samples here, which is especially easy to overfit.\n",
        "\n",
        "In the next lesson we will talk about how RNN is applied in natural language processing.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}