{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "# ResNet\n",
        "当大家还在惊叹 GoogLeNet 的 inception 结构的时候，微软亚洲研究院的研究员已经在设计更深但结构更加简单的网络 ResNet，并且凭借这个网络子在 2015 年 ImageNet 比赛上大获全胜。\n",
        "\n",
        "ResNet effectively solves the problem that deep neural networks are difficult to train and can train up to 1000 layers of convolutional networks. The reason why the network is difficult to train is because there is a problem that the gradient disappears. The farther away from the loss function, the smaller the gradient, the harder it is to update when the backpropagation is reversed. As the number of layers increases, the phenomenon becomes more serious. . There are two common scenarios to solve this problem before:\n",
        "\n",
        "1. Training by layer, first train the shallower layer, and then increase the number of layers, but this method is not particularly good, and it is more troublesome.\n",
        "\n",
        "2. Use a wider layer, or increase the output channel without deepening the number of layers in the network. This structure often does not work well.\n",
        "\n",
        "ResNet solves the problem of gradient backhaul disappearing by introducing cross-layer links.\n",
        "\n",
        "![](https://ws1.sinaimg.cn/large/006tNc79ly1fmptq2snv9j30j808t74a.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "This is a comparison of common network connections and cross-layer residuals. Using ordinary connections, the gradient of the upper layer must be transmitted layer by layer, but connected by residuals, which is equivalent to a shorter road in the middle. The gradient can be passed back from this shorter road, avoiding the situation where the gradient is too small.\n",
        "\n",
        "Suppose the input of a layer is x, and the expected output is H(x). If we pass the input x directly to the output as the initial result, this is a shallower network, which is easier to train, and the network has no learning part. We can use the deeper network F(x) to train it, making the training easier. Finally, we hope that the result of the fitting is F(x) = H(x) - x, which is the structure of a residual.\n",
        "\n",
        "The structure of the residual network is the stack of the residual blocks above. Let's implement a residual block.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T12:56:06.772059Z",
          "start_time": "2017-12-22T12:56:06.766027Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchvision.datasets import CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T12:47:49.222432Z",
          "start_time": "2017-12-22T12:47:49.217940Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def conv3x3(in_channel, out_channel, stride=1):\n",
        "    return nn.Conv2d(in_channel, out_channel, 3, stride=stride, padding=1, bias=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T13:14:02.429145Z",
          "start_time": "2017-12-22T13:14:02.383322Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "class residual_block(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, same_shape=True):\n",
        "        super(residual_block, self).__init__()\n",
        "        self.same_shape = same_shape\n",
        "        stride=1 if self.same_shape else 2\n",
        "        \n",
        "        self.conv1 = conv3x3(in_channel, out_channel, stride=stride)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
        "        \n",
        "        self.conv2 = conv3x3(out_channel, out_channel)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
        "        if not self.same_shape:\n",
        "            self.conv3 = nn.Conv2d(in_channel, out_channel, 1, stride=stride)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = F.relu(self.bn1(out), True)\n",
        "        out = self.conv2(out)\n",
        "        out = F.relu(self.bn2(out), True)\n",
        "        \n",
        "        if not self.same_shape:\n",
        "            x = self.conv3(x)\n",
        "        return F.relu(x+out, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Let's test the input and output of a residual block.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T13:14:05.793185Z",
          "start_time": "2017-12-22T13:14:05.763382Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input: torch.Size([1, 32, 96, 96])\n",
            "output: torch.Size([1, 32, 96, 96])\n"
          ]
        }
      ],
      "source": [
        "# Input output shape is the same\n",
        "test_net = residual_block(32, 32)\n",
        "test_x = Variable(torch.zeros(1, 32, 96, 96))\n",
        "print('input: {}'.format(test_x.shape))\n",
        "test_y = test_net(test_x)\n",
        "print('output: {}'.format(test_y.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T13:14:11.929120Z",
          "start_time": "2017-12-22T13:14:11.914604Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input: torch.Size([1, 3, 96, 96])\n",
            "output: torch.Size([1, 32, 48, 48])\n"
          ]
        }
      ],
      "source": [
        "# Input output shape is different\n",
        "test_net = residual_block(3, 32, False)\n",
        "test_x = Variable(torch.zeros(1, 3, 96, 96))\n",
        "print('input: {}'.format(test_x.shape))\n",
        "test_y = test_net(test_x)\n",
        "print('output: {}'.format(test_y.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Let's try to implement a ResNet, which is the stack of the residual block module.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T13:27:46.099404Z",
          "start_time": "2017-12-22T13:27:45.986235Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "class resnet(nn.Module):\n",
        "    def __init__(self, in_channel, num_classes, verbose=False):\n",
        "        super(resnet, self).__init__()\n",
        "        self.verbose = verbose\n",
        "        \n",
        "        self.block1 = nn.Conv2d(in_channel, 64, 7, 2)\n",
        "        \n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.MaxPool2d(3, 2),\n",
        "            residual_block(64, 64),\n",
        "            residual_block(64, 64)\n",
        "        )\n",
        "        \n",
        "        self.block3 = nn.Sequential(\n",
        "            residual_block(64, 128, False),\n",
        "            residual_block(128, 128)\n",
        "        )\n",
        "        \n",
        "        self.block4 = nn.Sequential(\n",
        "            residual_block(128, 256, False),\n",
        "            residual_block(256, 256)\n",
        "        )\n",
        "        \n",
        "        self.block5 = nn.Sequential(\n",
        "            residual_block(256, 512, False),\n",
        "            residual_block(512, 512),\n",
        "            nn.AvgPool2d(3)\n",
        "        )\n",
        "        \n",
        "        self.classifier = nn.Linear(512, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        if self.verbose:\n",
        "            print('block 1 output: {}'.format(x.shape))\n",
        "        x = self.block2(x)\n",
        "        if self.verbose:\n",
        "            print('block 2 output: {}'.format(x.shape))\n",
        "        x = self.block3(x)\n",
        "        if self.verbose:\n",
        "            print('block 3 output: {}'.format(x.shape))\n",
        "        x = self.block4(x)\n",
        "        if self.verbose:\n",
        "            print('block 4 output: {}'.format(x.shape))\n",
        "        x = self.block5(x)\n",
        "        if self.verbose:\n",
        "            print('block 5 output: {}'.format(x.shape))\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Output the size after each block\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T13:28:00.597030Z",
          "start_time": "2017-12-22T13:28:00.417746Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "block 1 output: torch.Size([1, 64, 45, 45])\n",
            "block 2 output: torch.Size([1, 64, 22, 22])\n",
            "block 3 output: torch.Size([1, 128, 11, 11])\n",
            "block 4 output: torch.Size([1, 256, 6, 6])\n",
            "block 5 output: torch.Size([1, 512, 1, 1])\n",
            "output: torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "test_net = resnet(3, 10, True)\n",
        "test_x = Variable(torch.zeros(1, 3, 96, 96))\n",
        "test_y = test_net(test_x)\n",
        "print('output: {}'.format(test_y.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T13:29:01.484172Z",
          "start_time": "2017-12-22T13:29:00.095952Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from utils import train\n",
        "\n",
        "def data_tf(x):\n",
        "x = x.resize((96, 96), 2) # Enlarge the image to 96 x 96\n",
        "    x = np.array(x, dtype='float32') / 255\n",
        "x = (x - 0.5)\n",
        "x = x.transpose((2, 0, 1)) # Put the channel in the first dimension, just the input method required by pytorch\n",
        "    x = torch.from_numpy(x)\n",
        "    return x\n",
        "     \n",
        "train_set = CIFAR10('./data', train=True, transform=data_tf)\n",
        "train_data = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "test_set = CIFAR10('./data', train=False, transform=data_tf)\n",
        "test_data = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False)\n",
        "\n",
        "net = resnet(3, 10)\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T13:45:00.783186Z",
          "start_time": "2017-12-22T13:29:09.214453Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0. Train Loss: 1.437317, Train Acc: 0.476662, Valid Loss: 1.928288, Valid Acc: 0.384691, Time 00:00:44\n",
            "Epoch 1. Train Loss: 0.992832, Train Acc: 0.648198, Valid Loss: 1.009847, Valid Acc: 0.642405, Time 00:00:48\n",
            "Epoch 2. Train Loss: 0.767309, Train Acc: 0.732617, Valid Loss: 1.827319, Valid Acc: 0.430380, Time 00:00:47\n",
            "Epoch 3. Train Loss: 0.606737, Train Acc: 0.788043, Valid Loss: 1.304808, Valid Acc: 0.585245, Time 00:00:46\n",
            "Epoch 4. Train Loss: 0.484436, Train Acc: 0.834499, Valid Loss: 1.335749, Valid Acc: 0.617089, Time 00:00:47\n",
            "Epoch 5. Train Loss: 0.374320, Train Acc: 0.872922, Valid Loss: 0.878519, Valid Acc: 0.724288, Time 00:00:47\n",
            "Epoch 6. Train Loss: 0.280981, Train Acc: 0.904212, Valid Loss: 0.931616, Valid Acc: 0.716871, Time 00:00:48\n",
            "Epoch 7. Train Loss: 0.210800, Train Acc: 0.929747, Valid Loss: 1.448870, Valid Acc: 0.638548, Time 00:00:48\n",
            "Epoch 8. Train Loss: 0.147873, Train Acc: 0.951427, Valid Loss: 1.356992, Valid Acc: 0.657536, Time 00:00:47\n",
            "Epoch 9. Train Loss: 0.112824, Train Acc: 0.963895, Valid Loss: 1.630560, Valid Acc: 0.627769, Time 00:00:47\n",
            "Epoch 10. Train Loss: 0.082685, Train Acc: 0.973905, Valid Loss: 0.982882, Valid Acc: 0.744264, Time 00:00:44\n",
            "Epoch 11. Train Loss: 0.065325, Train Acc: 0.979680, Valid Loss: 0.911631, Valid Acc: 0.767009, Time 00:00:47\n",
            "Epoch 12. Train Loss: 0.041401, Train Acc: 0.987952, Valid Loss: 1.167992, Valid Acc: 0.729826, Time 00:00:48\n",
            "Epoch 13. Train Loss: 0.037516, Train Acc: 0.989011, Valid Loss: 1.081807, Valid Acc: 0.746737, Time 00:00:47\n",
            "Epoch 14. Train Loss: 0.030674, Train Acc: 0.991468, Valid Loss: 0.935292, Valid Acc: 0.774031, Time 00:00:45\n",
            "Epoch 15. Train Loss: 0.021743, Train Acc: 0.994565, Valid Loss: 0.879348, Valid Acc: 0.790150, Time 00:00:47\n",
            "Epoch 16. Train Loss: 0.014642, Train Acc: 0.996463, Valid Loss: 1.328587, Valid Acc: 0.724387, Time 00:00:47\n",
            "Epoch 17. Train Loss: 0.011072, Train Acc: 0.997363, Valid Loss: 0.909065, Valid Acc: 0.792919, Time 00:00:47\n",
            "Epoch 18. Train Loss: 0.006870, Train Acc: 0.998561, Valid Loss: 0.923746, Valid Acc: 0.794403, Time 00:00:46\n",
            "Epoch 19. Train Loss: 0.004240, Train Acc: 0.999500, Valid Loss: 0.877908, Valid Acc: 0.802314, Time 00:00:46\n"
          ]
        }
      ],
      "source": [
        "train(net, train_data, test_data, 20, optimizer, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "ResNet uses cross-layer channels to make it possible to train very deep convolutional neural networks. It also uses a very simple convolutional layer configuration to make it easier to extend.\n",
        "\n",
        "**Little exercises:\n",
        "1. Try the structure of the bottleneck proposed in the paper.\n",
        "2. Try changing the order of conv -> bn -> relu to bn -> relu -> conv, see if the precision will improve **\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}