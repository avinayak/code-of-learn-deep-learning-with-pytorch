{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "#批标准\n",
        "在我们正式进入模型的构建和训练之前，我们会先讲一讲数据预处理和批标准化，因为模型训练并不容易，特别是一些非常复杂的模型，并不能非常好的训练得到收敛的结果，所以对数据增加一些预处理，同时使用批标准化能够得到非常好的收敛结果，这也是卷积网络能够训练到非常深的层的一个重要原因。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "## Data preprocessing\n",
        "At present, the most common method of data preprocessing is centralized and standardized. The centralization is equivalent to correcting the center position of the data. The implementation method is very simple, that is, the corresponding mean is subtracted from each feature dimension, and finally the feature of 0 mean is obtained. Standardization is also very simple. After the data becomes zero mean, in order to make the different feature dimensions have the same scale, the standard deviation can be divided into a standard normal distribution, or it can be converted into - according to the maximum and minimum values. Between 1 and 1, below is a simple icon\n",
        "\n",
        "![](https://ws1.sinaimg.cn/large/006tKfTcly1fmqouzer3xj30ij06n0t8.jpg)\n",
        "\n",
        "These two methods are very common. If you remember, we used this method to standardize the data in the neural network part. As for other methods, such as PCA or white noise, it has been used very little.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "## Batch Normalization\n",
        "In the previous data preprocessing, we try to input a normal distribution whose characteristics are irrelevant and satisfy a standard, so that the performance of the model is generally better. But for deep network structures, the nonlinear layer of the network makes the output results relevant and no longer satisfies a standard N(0, 1) distribution, even the center of the output has shifted. This is very difficult for the training of the model, especially the deep model training.\n",
        "\n",
        "所以在 2015 年一篇论文提出了这个方法，批标准化，简而言之，就是对于每一层网络的输出，对其做一个归一化，使其服从标准的正态分布，这样后一层网络的输入也是一个标准的正态分布，所以能够比较好的进行训练，加快收敛速度。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "The implementation of batch normalization is very simple, for a given batch of data, the formula for the $B = \\{x_1, x_2, \\cdots, x_m\\}$ algorithm is as follows\n",
        "\n",
        "$$\n",
        "\\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i\n",
        "$$\n",
        "$$\n",
        "\\sigma^2_B = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2\n",
        "$$\n",
        "$$\n",
        "\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma^2_B + \\epsilon}}\n",
        "$$\n",
        "$$\n",
        "y_i = \\gamma \\hat{x}_i + \\beta\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "The first and second lines calculate the mean and variance of the data in a batch, and then use the third formula to normalize each data point in the batch. $\\epsilon$ is a small constant introduced to calculate stability. Usually take $10^{-5}$, and finally use the weight correction to get the final output. It is very simple. Below we can implement a simple one-dimensional situation, that is, the situation in the neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-23T06:50:51.579067Z",
          "start_time": "2017-12-23T06:50:51.575693Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-23T07:14:11.077807Z",
          "start_time": "2017-12-23T07:14:11.060849Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def simple_batch_norm_1d(x, gamma, beta):\n",
        "    eps = 1e-5\n",
        "X_mean = torch.mean(x, dim=0, keepdim=True) # Reserved dimension for broadcast\n",
        "    x_var = torch.mean((x - x_mean) ** 2, dim=0, keepdim=True)\n",
        "    x_hat = (x - x_mean) / torch.sqrt(x_var + eps)\n",
        "    return gamma.view_as(x_mean) * x_hat + beta.view_as(x_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Let's verify if the output is normalized for any input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-23T07:14:20.610603Z",
          "start_time": "2017-12-23T07:14:20.597682Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "before bn: \n",
            "\n",
            "  0   1   2\n",
            "  3   4   5\n",
            "  6   7   8\n",
            "  9  10  11\n",
            " 12  13  14\n",
            "[torch.FloatTensor of size 5x3]\n",
            "\n",
            "after bn: \n",
            "\n",
            "-1.4142 -1.4142 -1.4142\n",
            "-0.7071 -0.7071 -0.7071\n",
            " 0.0000  0.0000  0.0000\n",
            " 0.7071  0.7071  0.7071\n",
            " 1.4142  1.4142  1.4142\n",
            "[torch.FloatTensor of size 5x3]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "x = torch.arange(15).view(5, 3)\n",
        "gamma = torch.ones(x.shape[1])\n",
        "beta = torch.zeros(x.shape[1])\n",
        "print('before bn: ')\n",
        "print(x)\n",
        "y = simple_batch_norm_1d(x, gamma, beta)\n",
        "print('after bn: ')\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "It can be seen that there are a total of 5 data points, three features, each column representing a different data point of a feature. After batch normalization, each column becomes a standard normal distribution.\n",
        "\n",
        "There will be a problem at this time, is it to use batch standardization when testing?\n",
        "\n",
        "The answer is yes, because it is used during training, and the use of the test will definitely lead to deviations in the results, but if there is only one data set in the test, then the mean is not this value, the variance is 0? This is obviously random, so you can't use the test data set to calculate the mean and variance when testing, but instead use the moving average and variance calculated during training.\n",
        "\n",
        "Below we implement the following batch standardization method that can distinguish between training state and test state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-23T07:32:48.025709Z",
          "start_time": "2017-12-23T07:32:48.005892Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def batch_norm_1d(x, gamma, beta, is_training, moving_mean, moving_var, moving_momentum=0.1):\n",
        "    eps = 1e-5\n",
        "X_mean = torch.mean(x, dim=0, keepdim=True) # Reserved dimension for broadcast\n",
        "    x_var = torch.mean((x - x_mean) ** 2, dim=0, keepdim=True)\n",
        "    if is_training:\n",
        "        x_hat = (x - x_mean) / torch.sqrt(x_var + eps)\n",
        "        moving_mean[:] = moving_momentum * moving_mean + (1. - moving_momentum) * x_mean\n",
        "        moving_var[:] = moving_momentum * moving_var + (1. - moving_momentum) * x_var\n",
        "    else:\n",
        "        x_hat = (x - moving_mean) / torch.sqrt(moving_var + eps)\n",
        "    return gamma.view_as(x_mean) * x_hat + beta.view_as(x_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Below we use the example of the deep neural network classification mnist dataset from the previous lesson to test whether batch standardization is useful.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "From torchvision.datasets import mnist # import pytorch built-in mnist data\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        
      },
      "outputs": [],
      "source": [
        "# Download the mnist dataset using built-in functions\n",
        "train_set = mnist.MNIST('./data', train=True)\n",
        "test_set = mnist.MNIST('./data', train=False)\n",
        "\n",
        "def data_tf(x):\n",
        "    x = np.array(x, dtype='float32') / 255\n",
        "x = (x - 0.5)\n",
        "x = x.reshape((-1,)) #拉平\n",
        "    x = torch.from_numpy(x)\n",
        "    return x\n",
        "\n",
        "train_set = mnist.MNIST('.\n",
        "test_set = mnist.MNIST('./data', train=False, transform=data_tf, download=True)\n",
        "train_data = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "test_data = DataLoader(test_set, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "class multi_network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(multi_network, self).__init__()\n",
        "        self.layer1 = nn.Linear(784, 100)\n",
        "        self.relu = nn.ReLU(True)\n",
        "        self.layer2 = nn.Linear(100, 10)\n",
        "        \n",
        "        self.gamma = nn.Parameter(torch.randn(100))\n",
        "        self.beta = nn.Parameter(torch.randn(100))\n",
        "        \n",
        "        self.moving_mean = Variable(torch.zeros(100))\n",
        "        self.moving_var = Variable(torch.zeros(100))\n",
        "        \n",
        "    def forward(self, x, is_train=True):\n",
        "        x = self.layer1(x)\n",
        "        x = batch_norm_1d(x, self.gamma, self.beta, is_train, self.moving_mean, self.moving_var)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "net = multi_network()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        
      },
      "outputs": [],
      "source": [
        "# define loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "Optimizer = torch.optim.SGD(net.parameters(), 1e-1) # Use random gradient descent, learning rate 0.1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "For convenience, the training function has been defined in the outside utils.py, the same as the previous training network operation, interested students can go and see\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0. Train Loss: 0.308139, Train Acc: 0.912797, Valid Loss: 0.181375, Valid Acc: 0.948279, Time 00:00:07\n",
            "Epoch 1. Train Loss: 0.174049, Train Acc: 0.949910, Valid Loss: 0.143940, Valid Acc: 0.958267, Time 00:00:09\n",
            "Epoch 2. Train Loss: 0.134983, Train Acc: 0.961587, Valid Loss: 0.122489, Valid Acc: 0.963904, Time 00:00:08\n",
            "Epoch 3. Train Loss: 0.111758, Train Acc: 0.968317, Valid Loss: 0.106595, Valid Acc: 0.966278, Time 00:00:09\n",
            "Epoch 4. Train Loss: 0.096425, Train Acc: 0.971915, Valid Loss: 0.108423, Valid Acc: 0.967563, Time 00:00:10\n",
            "Epoch 5. Train Loss: 0.084424, Train Acc: 0.974464, Valid Loss: 0.107135, Valid Acc: 0.969838, Time 00:00:09\n",
            "Epoch 6. Train Loss: 0.076206, Train Acc: 0.977645, Valid Loss: 0.092725, Valid Acc: 0.971420, Time 00:00:09\n",
            "Epoch 7. Train Loss: 0.069438, Train Acc: 0.979661, Valid Loss: 0.091497, Valid Acc: 0.971519, Time 00:00:09\n",
            "Epoch 8. Train Loss: 0.062908, Train Acc: 0.980810, Valid Loss: 0.088797, Valid Acc: 0.972903, Time 00:00:08\n",
            "Epoch 9. Train Loss: 0.058186, Train Acc: 0.982309, Valid Loss: 0.090830, Valid Acc: 0.972310, Time 00:00:08\n"
          ]
        }
      ],
      "source": [
        "from utils import train\n",
        "train(net, train_data, test_data, 10, optimizer, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Here, both $\\gamma$ and $\\beta$ are trained as parameters, initialized to a random Gaussian distribution, and both `moving_mean` and `moving_var` are initialized to 0, not updated parameters. After 10 training sessions, we can See how the moving average and moving variance are modified\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variable containing:\n",
            " 0.5505\n",
            " 2.0835\n",
            " 0.0794\n",
            "-0.1991\n",
            "-0.9822\n",
            "-0.5820\n",
            " 0.6991\n",
            "-0.1292\n",
            " 2.9608\n",
            " 1.0826\n",
            "[torch.FloatTensor of size 10]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 打出 the top 10 items of moving_mean\n",
        "print(net.moving_mean[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "It can be seen that these values have been modified during the training process. During the test, we do not need to calculate the mean and variance, and we can directly use the moving average and the moving variance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "For comparison, let's look at the results of not using batch normalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0. Train Loss: 0.402263, Train Acc: 0.873817, Valid Loss: 0.220468, Valid Acc: 0.932852, Time 00:00:07\n",
            "Epoch 1. Train Loss: 0.181916, Train Acc: 0.945379, Valid Loss: 0.162440, Valid Acc: 0.953817, Time 00:00:08\n",
            "Epoch 2. Train Loss: 0.136073, Train Acc: 0.958522, Valid Loss: 0.264888, Valid Acc: 0.918216, Time 00:00:08\n",
            "Epoch 3. Train Loss: 0.111658, Train Acc: 0.966551, Valid Loss: 0.149704, Valid Acc: 0.950752, Time 00:00:08\n",
            "Epoch 4. Train Loss: 0.096433, Train Acc: 0.970732, Valid Loss: 0.116364, Valid Acc: 0.963311, Time 00:00:07\n",
            "Epoch 5. Train Loss: 0.083800, Train Acc: 0.973914, Valid Loss: 0.105775, Valid Acc: 0.968058, Time 00:00:08\n",
            "Epoch 6. Train Loss: 0.074534, Train Acc: 0.977129, Valid Loss: 0.094511, Valid Acc: 0.970728, Time 00:00:08\n",
            "Epoch 7. Train Loss: 0.067365, Train Acc: 0.979311, Valid Loss: 0.130495, Valid Acc: 0.960146, Time 00:00:09\n",
            "Epoch 8. Train Loss: 0.061585, Train Acc: 0.980894, Valid Loss: 0.089632, Valid Acc: 0.974090, Time 00:00:08\n",
            "Epoch 9. Train Loss: 0.055352, Train Acc: 0.982892, Valid Loss: 0.091508, Valid Acc: 0.970431, Time 00:00:08\n"
          ]
        }
      ],
      "source": [
        "no_bn_net = nn.Sequential(\n",
        "    nn.Linear(784, 100),\n",
        "    nn.ReLU(True),\n",
        "    nn.Linear(100, 10)\n",
        ")\n",
        "\n",
        "Optimizer = torch.optim.SGD(no_bn_net.parameters(), 1e-1) # Use random gradient descent, learning rate 0.1\n",
        "train(no_bn_net, train_data, test_data, 10, optimizer, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "It can be seen that although the final result is the same in both cases, if we look at the previous situation, we can see that the use of batch standardization can converge more quickly, because this is just a small network, so it can be used without batch standardization. Convergence, but for deeper networks, using batch normalization can converge quickly during training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "As you can see from the above, we have implemented batch normalization of the 2-dimensional case. The standardization of the 4-dimensional case corresponding to the convolution is similar. We only need to calculate the mean and variance along the dimensions of the channel, but we implement the batch ourselves. Standardization is very tiring, and pytorch of course also has built-in batch-normalized functions for us. One-dimensional and two-dimensional are `torch.nn.BatchNorm1d()` and `torch.nn.BatchNorm2d()`, which are different from our implementation. Pytorch not only uses $\\gamma$ and $\\beta$ as training parameters, but also `moving_mean` and `moving_var` as parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Let's try the batch standardization under the convolution network to see the effect.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def data_tf(x):\n",
        "    x = np.array(x, dtype='float32') / 255\n",
        "x = (x - 0.5)\n",
        "    x = torch.from_numpy(x)\n",
        "    x = x.unsqueeze(0)\n",
        "    return x\n",
        "\n",
        "train_set = mnist.MNIST('.\n",
        "test_set = mnist.MNIST('./data', train=False, transform=data_tf, download=True)\n",
        "train_data = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "test_data = DataLoader(test_set, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        
      },
      "outputs": [],
      "source": [
        "#用批标准\n",
        "class conv_bn_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(conv_bn_net, self).__init__()\n",
        "        self.stage1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 6, 3, padding=1),\n",
        "            nn.BatchNorm2d(6),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(6, 16, 5),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        \n",
        "        self.classfy = nn.Linear(400, 10)\n",
        "    def forward(self, x):\n",
        "        x = self.stage1(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.classfy(x)\n",
        "        return x\n",
        "\n",
        "net = conv_bn_net()\n",
        "Optimizer = torch.optim.SGD(net.parameters(), 1e-1) # Use random gradient descent, learning rate 0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0. Train Loss: 0.160329, Train Acc: 0.952842, Valid Loss: 0.063328, Valid Acc: 0.978441, Time 00:00:33\n",
            "Epoch 1. Train Loss: 0.067862, Train Acc: 0.979361, Valid Loss: 0.068229, Valid Acc: 0.979430, Time 00:00:37\n",
            "Epoch 2. Train Loss: 0.051867, Train Acc: 0.984625, Valid Loss: 0.044616, Valid Acc: 0.985265, Time 00:00:37\n",
            "Epoch 3. Train Loss: 0.044797, Train Acc: 0.986141, Valid Loss: 0.042711, Valid Acc: 0.986056, Time 00:00:38\n",
            "Epoch 4. Train Loss: 0.039876, Train Acc: 0.987690, Valid Loss: 0.042499, Valid Acc: 0.985067, Time 00:00:41\n"
          ]
        }
      ],
      "source": [
        "train(net, train_data, test_data, 5, optimizer, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Do not use batch standardization\n",
        "class conv_no_bn_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(conv_no_bn_net, self).__init__()\n",
        "        self.stage1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 6, 3, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(6, 16, 5),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        \n",
        "        self.classfy = nn.Linear(400, 10)\n",
        "    def forward(self, x):\n",
        "        x = self.stage1(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.classfy(x)\n",
        "        return x\n",
        "\n",
        "net = conv_no_bn_net()\n",
        "Optimizer = torch.optim.SGD(net.parameters(), 1e-1) # Use random gradient descent, learning rate 0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0. Train Loss: 0.211075, Train Acc: 0.935934, Valid Loss: 0.062950, Valid Acc: 0.980123, Time 00:00:27\n",
            "Epoch 1. Train Loss: 0.066763, Train Acc: 0.978778, Valid Loss: 0.050143, Valid Acc: 0.984375, Time 00:00:29\n",
            "Epoch 2. Train Loss: 0.050870, Train Acc: 0.984292, Valid Loss: 0.039761, Valid Acc: 0.988034, Time 00:00:29\n",
            "Epoch 3. Train Loss: 0.041476, Train Acc: 0.986924, Valid Loss: 0.041925, Valid Acc: 0.986155, Time 00:00:29\n",
            "Epoch 4. Train Loss: 0.036118, Train Acc: 0.988523, Valid Loss: 0.042703, Valid Acc: 0.986452, Time 00:00:29\n"
          ]
        }
      ],
      "source": [
        "train(net, train_data, test_data, 5, optimizer, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "When we introduce some famous network structures, we will gradually realize the importance of batch standardization. It is very convenient to add batch standardization layer by using pytorch.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}